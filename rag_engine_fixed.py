#!/usr/bin/env python
"""
ä¿®å¤ç‰ˆæœ¬çš„ RAG å¼•æ“ - è§£å†³ transformers åº“ NoneType é”™è¯¯
åŒ…å«å¤šç§è§£å†³æ–¹æ¡ˆçš„ RAG æ ¸å¿ƒå®ç°
"""

import os
import sys
import torch
import gc
from pathlib import Path
from typing import List, Dict, Any, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM

class FixedRAGEngine:
    """ä¿®å¤ç‰ˆæœ¬çš„ RAG å¼•æ“"""
    
    def __init__(self, config):
        """åˆå§‹åŒ–RAGå¼•æ“"""
        self.config = config
        self.llm = None
        self.tokenizer = None
        self.embedding_model = None
        self.index = None
        self.query_engine = None
        
        # åº”ç”¨ä¿®å¤è¡¥ä¸
        self._apply_transformers_fix()
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        self._set_environment_variables()
    
    def _apply_transformers_fix(self):
        """åº”ç”¨ transformers åº“ä¿®å¤è¡¥ä¸"""
        print("ğŸ”§ åº”ç”¨ transformers åº“ä¿®å¤è¡¥ä¸...")
        
        try:
            import transformers.modeling_utils as modeling_utils
            
            # ä¿®å¤ ALL_PARALLEL_STYLES
            if hasattr(modeling_utils, 'ALL_PARALLEL_STYLES'):
                if modeling_utils.ALL_PARALLEL_STYLES is None:
                    print("âš ï¸  å‘ç° ALL_PARALLEL_STYLES ä¸º Noneï¼Œæ­£åœ¨ä¿®å¤...")
                    modeling_utils.ALL_PARALLEL_STYLES = [
                        "model_parallel",
                        "pipeline_parallel", 
                        "tensor_parallel",
                        "data_parallel"
                    ]
                    print("âœ… ALL_PARALLEL_STYLES ä¿®å¤å®Œæˆ")
                else:
                    print("âœ… ALL_PARALLEL_STYLES æ­£å¸¸")
            else:
                print("âš ï¸  æ·»åŠ  ALL_PARALLEL_STYLES å±æ€§...")
                modeling_utils.ALL_PARALLEL_STYLES = [
                    "model_parallel",
                    "pipeline_parallel", 
                    "tensor_parallel", 
                    "data_parallel"
                ]
                print("âœ… ALL_PARALLEL_STYLES æ·»åŠ å®Œæˆ")
                
        except Exception as e:
            print(f"âš ï¸  è¿è¡Œæ—¶è¡¥ä¸åº”ç”¨å¤±è´¥: {e}")
            print("å°†ç»§ç»­å°è¯•å…¶ä»–è§£å†³æ–¹æ¡ˆ...")
    
    def _set_environment_variables(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        print("ğŸ”§ è®¾ç½®ç¯å¢ƒå˜é‡...")
        
        # è®¾ç½® transformers ç›¸å…³ç¯å¢ƒå˜é‡
        os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        
        print("âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")
    
    def load_llm_with_retry(self):
        """ä½¿ç”¨é‡è¯•æœºåˆ¶åŠ è½½LLMæ¨¡å‹"""
        print(f"ğŸ¤– åŠ è½½LLMæ¨¡å‹: {self.config.LLM_MODEL_PATH}")
        
        # åŠ è½½ç­–ç•¥åˆ—è¡¨ï¼ˆä»æœ€å®‰å…¨åˆ°æœ€é«˜æ€§èƒ½ï¼‰
        load_strategies = [
            {
                "name": "æœ€å®‰å…¨æ¨¡å¼",
                "params": {
                    "torch_dtype": torch.float32,
                    "device_map": "cpu",
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True
                }
            },
            {
                "name": "CPUæ¨¡å¼",
                "params": {
                    "torch_dtype": torch.float16,
                    "device_map": "cpu", 
                    "trust_remote_code": True
                }
            },
            {
                "name": "å•GPUæ¨¡å¼",
                "params": {
                    "torch_dtype": torch.float16,
                    "device_map": "cuda:0",
                    "trust_remote_code": True
                }
            },
            {
                "name": "è‡ªåŠ¨è®¾å¤‡æ˜ å°„",
                "params": {
                    "torch_dtype": torch.float16,
                    "device_map": "auto",
                    "trust_remote_code": True
                }
            }
        ]
        
        # é¦–å…ˆåŠ è½½ tokenizer
        try:
            print("ğŸ“ åŠ è½½ tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.LLM_MODEL_PATH,
                trust_remote_code=True
            )
            print("âœ… tokenizer åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ tokenizer åŠ è½½å¤±è´¥: {e}")
            return False
        
        # å°è¯•ä¸åŒç­–ç•¥åŠ è½½æ¨¡å‹
        for strategy in load_strategies:
            try:
                print(f"ğŸ”„ å°è¯•ç­–ç•¥: {strategy['name']}")
                print(f"å‚æ•°: {strategy['params']}")
                
                # æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                
                # åŠ è½½æ¨¡å‹
                self.llm = AutoModelForCausalLM.from_pretrained(
                    self.config.LLM_MODEL_PATH,
                    **strategy['params']
                )
                
                print(f"âœ… ä½¿ç”¨ {strategy['name']} åŠ è½½æ¨¡å‹æˆåŠŸï¼")
                return True
                
            except Exception as e:
                print(f"âŒ ç­–ç•¥ {strategy['name']} å¤±è´¥: {e}")
                
                # å¦‚æœæ˜¯æˆ‘ä»¬è¦ä¿®å¤çš„ç‰¹å®šé”™è¯¯ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                if "NoneType" in str(e) and "iterable" in str(e):
                    print("ğŸ¯ è¿™æ˜¯æˆ‘ä»¬è¦ä¿®å¤çš„ NoneType é”™è¯¯ï¼")
                    print("æ­£åœ¨å°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥...")
                
                # æ¸…ç†å¤±è´¥çš„æ¨¡å‹
                if hasattr(self, 'llm') and self.llm is not None:
                    del self.llm
                    self.llm = None
                
                continue
        
        print("âŒ æ‰€æœ‰åŠ è½½ç­–ç•¥éƒ½å¤±è´¥äº†")
        return False
    
    def load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        print(f"ğŸ“Š åŠ è½½åµŒå…¥æ¨¡å‹: {self.config.EMBEDDING_MODEL_PATH}")
        
        try:
            self.embedding_model = HuggingFaceEmbedding(
                model_name=self.config.EMBEDDING_MODEL_PATH,
                trust_remote_code=True
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def initialize(self):
        """åˆå§‹åŒ– RAG å¼•æ“"""
        print("ğŸš€ åˆå§‹åŒ– RAG å¼•æ“...")
        
        # 1. åŠ è½½åµŒå…¥æ¨¡å‹
        if not self.load_embedding_model():
            return False
        
        # 2. åŠ è½½LLMæ¨¡å‹
        if not self.load_llm_with_retry():
            return False
        
        # 3. è®¾ç½®å…¨å±€é…ç½®
        Settings.embed_model = self.embedding_model
        
        print("âœ… RAG å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        return True
    
    def create_documents(self, texts: List[str]) -> List[Document]:
        """åˆ›å»ºæ–‡æ¡£å¯¹è±¡"""
        print(f"ğŸ“„ åˆ›å»º {len(texts)} ä¸ªæ–‡æ¡£...")
        documents = [Document(text=text) for text in texts]
        print("âœ… æ–‡æ¡£åˆ›å»ºå®Œæˆ")
        return documents
    
    def build_index(self, documents: List[Document]):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        print("ğŸ” æ„å»ºå‘é‡ç´¢å¼•...")
        
        try:
            self.index = VectorStoreIndex.from_documents(documents)
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=self.config.TOP_K,
                response_mode="compact"
            )
            print("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False
    
    def generate_answer(self, query: str, retrieved_texts: List[str]) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ"""
        try:
            # æ„å»ºæç¤ºè¯
            context = "\n\n".join(retrieved_texts)
            prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·æ ¹æ®æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”é—®é¢˜ï¼š"""
            
            # ä½¿ç”¨tokenizerç¼–ç 
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            
            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            if torch.cuda.is_available() and next(self.llm.parameters()).is_cuda:
                inputs = inputs.cuda()
            
            # ç”Ÿæˆç­”æ¡ˆ
            with torch.no_grad():
                outputs = self.llm.generate(
                    inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç ç­”æ¡ˆ
            answer = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            return answer.strip()
            
        except Exception as e:
            print(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}"
    
    def query(self, question: str) -> Dict[str, Any]:
        """æŸ¥è¯¢æ¥å£"""
        try:
            print(f"ğŸ” æŸ¥è¯¢: {question}")
            
            # ä½¿ç”¨æŸ¥è¯¢å¼•æ“æ£€ç´¢
            response = self.query_engine.query(question)
            
            # æå–æ£€ç´¢åˆ°çš„æ–‡æœ¬
            retrieved_texts = []
            if hasattr(response, 'source_nodes'):
                retrieved_texts = [node.text for node in response.source_nodes]
            
            # ä½¿ç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            if self.llm and self.tokenizer:
                answer = self.generate_answer(question, retrieved_texts)
            else:
                answer = str(response)
            
            return {
                "question": question,
                "answer": answer,
                "retrieved_texts": retrieved_texts,
                "confidence": 0.8  # ç®€å•çš„ç½®ä¿¡åº¦
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "question": question,
                "answer": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "retrieved_texts": [],
                "confidence": 0.0
            }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ§¹ æ¸…ç†èµ„æº...")
        
        if self.llm:
            del self.llm
            self.llm = None
        
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        
        if self.embedding_model:
            del self.embedding_model
            self.embedding_model = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")

# æµ‹è¯•å‡½æ•°
def test_fixed_rag_engine():
    """æµ‹è¯•ä¿®å¤ç‰ˆæœ¬çš„RAGå¼•æ“"""
    print("="*60)
    print("ğŸ§ª æµ‹è¯•ä¿®å¤ç‰ˆæœ¬çš„ RAG å¼•æ“")
    print("="*60)
    
    try:
        from config import Config
        
        # åˆ›å»ºRAGå¼•æ“
        rag_engine = FixedRAGEngine(Config)
        
        # åˆå§‹åŒ–
        if not rag_engine.initialize():
            print("âŒ RAGå¼•æ“åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # æµ‹è¯•æ–‡æ¡£
        test_documents = [
            "é“¶è¡Œèµ„æœ¬å……è¶³ç‡ç›‘ç®¡è¦æ±‚ï¼šæ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº5%ï¼Œä¸€çº§èµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº6%ï¼Œèµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº8%ã€‚",
            "ä¸ªäººç†è´¢äº§å“é”€å”®éœ€è¦è¿›è¡Œé£é™©è¯„ä¼°ï¼Œç¡®ä¿äº§å“é£é™©ç­‰çº§ä¸å®¢æˆ·é£é™©æ‰¿å—èƒ½åŠ›ç›¸åŒ¹é…ã€‚",
            "åŸå•†è¡Œå†…å®¡éƒ¨é—¨è´Ÿè´£äººä»»èŒåéœ€åœ¨5ä¸ªå·¥ä½œæ—¥å†…å‘ç›‘ç®¡éƒ¨é—¨å¤‡æ¡ˆã€‚"
        ]
        
        # åˆ›å»ºæ–‡æ¡£å¹¶æ„å»ºç´¢å¼•
        documents = rag_engine.create_documents(test_documents)
        if not rag_engine.build_index(documents):
            print("âŒ ç´¢å¼•æ„å»ºå¤±è´¥")
            return False
        
        # æµ‹è¯•æŸ¥è¯¢
        test_questions = [
            "é“¶è¡Œèµ„æœ¬å……è¶³ç‡çš„æœ€ä½è¦æ±‚æ˜¯å¤šå°‘ï¼Ÿ",
            "åŸå•†è¡Œå†…å®¡éƒ¨é—¨è´Ÿè´£äººä»»èŒåå¤šä¹…éœ€è¦å¤‡æ¡ˆï¼Ÿ"
        ]
        
        for question in test_questions:
            result = rag_engine.query(question)
            print(f"\né—®é¢˜: {result['question']}")
            print(f"ç­”æ¡ˆ: {result['answer']}")
            print(f"ç½®ä¿¡åº¦: {result['confidence']}")
        
        # æ¸…ç†èµ„æº
        rag_engine.cleanup()
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_fixed_rag_engine() 