#!/usr/bin/env python
"""
ä¿®å¤ transformers åº“ NoneType é”™è¯¯
è§£å†³æ–¹æ¡ˆï¼šä¸´æ—¶ä¿®è¡¥ transformers åº“ä¸­çš„ bug
"""

import os
import sys
import importlib.util
from pathlib import Path

def fix_transformers_bug():
    """ä¿®å¤ transformers åº“ä¸­çš„ ALL_PARALLEL_STYLES é—®é¢˜"""
    print("ğŸ”§ æ­£åœ¨ä¿®å¤ transformers åº“ bug...")
    
    try:
        # å¯¼å…¥ transformers
        import transformers
        from transformers.modeling_utils import ALL_PARALLEL_STYLES
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é—®é¢˜
        if ALL_PARALLEL_STYLES is None:
            print("âš ï¸  å‘ç° ALL_PARALLEL_STYLES ä¸º Noneï¼Œæ­£åœ¨ä¿®å¤...")
            
            # è®¾ç½®æ­£ç¡®çš„å¹¶è¡Œæ ·å¼
            import transformers.modeling_utils as modeling_utils
            modeling_utils.ALL_PARALLEL_STYLES = [
                "model_parallel",
                "pipeline_parallel", 
                "tensor_parallel",
                "data_parallel"
            ]
            
            print("âœ… ä¿®å¤å®Œæˆ")
            return True
        else:
            print("âœ… ALL_PARALLEL_STYLES æ­£å¸¸ï¼Œæ— éœ€ä¿®å¤")
            return True
            
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        return False

def alternative_fix():
    """æ›¿ä»£æ–¹æ¡ˆï¼šç›´æ¥ä¿®è¡¥ modeling_utils.py æ–‡ä»¶"""
    print("\nğŸ”§ å°è¯•æ›¿ä»£ä¿®å¤æ–¹æ¡ˆ...")
    
    try:
        import transformers
        
        # è·å– transformers å®‰è£…è·¯å¾„
        transformers_path = Path(transformers.__file__).parent
        modeling_utils_path = transformers_path / "modeling_utils.py"
        
        print(f"ğŸ“ transformers è·¯å¾„: {modeling_utils_path}")
        
        if modeling_utils_path.exists():
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(modeling_utils_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤
            if 'ALL_PARALLEL_STYLES = None' in content or 'ALL_PARALLEL_STYLES=None' in content:
                print("âš ï¸  å‘ç°é—®é¢˜ä»£ç ï¼Œæ­£åœ¨ä¿®å¤...")
                
                # æ›¿æ¢é—®é¢˜ä»£ç 
                content = content.replace(
                    'ALL_PARALLEL_STYLES = None',
                    'ALL_PARALLEL_STYLES = ["model_parallel", "pipeline_parallel", "tensor_parallel", "data_parallel"]'
                )
                content = content.replace(
                    'ALL_PARALLEL_STYLES=None',
                    'ALL_PARALLEL_STYLES=["model_parallel", "pipeline_parallel", "tensor_parallel", "data_parallel"]'
                )
                
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_path = modeling_utils_path.with_suffix('.py.backup')
                modeling_utils_path.rename(backup_path)
                print(f"ğŸ“¦ å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_path}")
                
                # å†™å…¥ä¿®å¤åçš„å†…å®¹
                with open(modeling_utils_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print("âœ… æ–‡ä»¶ä¿®å¤å®Œæˆ")
                return True
            else:
                print("âœ… æ–‡ä»¶ä¸­æœªå‘ç°é—®é¢˜ä»£ç ")
                return True
                
    except Exception as e:
        print(f"âŒ æ›¿ä»£ä¿®å¤å¤±è´¥: {e}")
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½æ˜¯å¦æ­£å¸¸"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        model_path = "/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-7B-Instruct"
        
        print("æ­¥éª¤1: åŠ è½½ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… tokenizer åŠ è½½æˆåŠŸ")
        
        print("æ­¥éª¤2: åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # æ¸…ç†å†…å­˜
        del model
        del tokenizer
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ› ä¿®å¤ transformers åº“ NoneType é”™è¯¯")
    print("="*60)
    
    # æ–¹æ¡ˆ1ï¼šè¿è¡Œæ—¶ä¿®å¤
    if fix_transformers_bug():
        print("âœ… è¿è¡Œæ—¶ä¿®å¤æˆåŠŸï¼Œæµ‹è¯•æ¨¡å‹åŠ è½½...")
        if test_model_loading():
            print("ğŸ‰ é—®é¢˜å·²è§£å†³ï¼")
            return
    
    # æ–¹æ¡ˆ2ï¼šæ–‡ä»¶ä¿®å¤
    print("\næ–¹æ¡ˆ1å¤±è´¥ï¼Œå°è¯•æ–¹æ¡ˆ2...")
    if alternative_fix():
        print("âœ… æ–‡ä»¶ä¿®å¤æˆåŠŸï¼Œè¯·é‡æ–°å¯¼å…¥ transformers åæµ‹è¯•")
        return
    
    # æ–¹æ¡ˆ3ï¼šç‰ˆæœ¬è§£å†³æ–¹æ¡ˆ
    print("\n" + "="*60)
    print("ğŸ” æ¨èè§£å†³æ–¹æ¡ˆ")
    print("="*60)
    print("1. å‡çº§ transformers ç‰ˆæœ¬:")
    print("   pip install --upgrade transformers")
    print()
    print("2. æˆ–è€…é™çº§åˆ°ç¨³å®šç‰ˆæœ¬:")
    print("   pip install transformers==4.44.0")
    print()
    print("3. è®¾ç½®ç¯å¢ƒå˜é‡:")
    print("   export TRANSFORMERS_VERBOSITY=error")
    print()
    print("4. æ‰‹åŠ¨ä¿®å¤ (è¿è¡Œæ­¤è„šæœ¬):")
    print("   python fix_transformers_issue.py")

if __name__ == "__main__":
    main() 