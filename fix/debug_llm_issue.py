#!/usr/bin/env python
"""
ç²¾ç¡®è°ƒè¯•LLMåŠ è½½é—®é¢˜
æ•è·è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
"""

import traceback
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from config import Config

def test_exact_same_code_as_rag():
    """ä½¿ç”¨ä¸rag_engine.pyå®Œå…¨ç›¸åŒçš„ä»£ç è¿›è¡Œæµ‹è¯•"""
    print("ğŸ” ä½¿ç”¨ä¸RAGå¼•æ“å®Œå…¨ç›¸åŒçš„ä»£ç æµ‹è¯•...")
    
    model_path = Config.LLM_MODEL_PATH
    model = None
    tokenizer = None
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è®¾å¤‡: {device}")
    
    try:
        print(f"åŠ è½½LLMæ¨¡å‹: {model_path}")
        
        # æ­¥éª¤1ï¼šåŠ è½½tokenizer
        print("æ­¥éª¤1: åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… tokenizeråŠ è½½æˆåŠŸ")
        
        # æ­¥éª¤2ï¼šåŠ è½½æ¨¡å‹
        print("æ­¥éª¤2: åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        return True, model, tokenizer
        
    except Exception as e:
        print(f"âŒ LLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        return False, None, None

def test_step_by_step():
    """é€æ­¥æµ‹è¯•ï¼Œæ‰¾å‡ºå…·ä½“åœ¨å“ªä¸€æ­¥å¤±è´¥"""
    print("\nğŸ”§ é€æ­¥æµ‹è¯•...")
    
    model_path = Config.LLM_MODEL_PATH
    
    # æµ‹è¯•1ï¼šåŸºç¡€å¯¼å…¥
    try:
        print("æµ‹è¯•1: å¯¼å…¥transformers...")
        from transformers import AutoTokenizer, AutoModelForCausalLM
        print("âœ… å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        traceback.print_exc()
        return
    
    # æµ‹è¯•2ï¼šæ£€æŸ¥è·¯å¾„
    try:
        print("æµ‹è¯•2: æ£€æŸ¥æ¨¡å‹è·¯å¾„...")
        from pathlib import Path
        if Path(model_path).exists():
            print("âœ… è·¯å¾„å­˜åœ¨")
        else:
            print("âŒ è·¯å¾„ä¸å­˜åœ¨")
            return
    except Exception as e:
        print(f"âŒ è·¯å¾„æ£€æŸ¥å¤±è´¥: {e}")
        traceback.print_exc()
        return
    
    # æµ‹è¯•3ï¼šåŠ è½½tokenizerï¼ˆä¸ä¼ é€’é¢å¤–å‚æ•°ï¼‰
    try:
        print("æµ‹è¯•3: åŠ è½½tokenizerï¼ˆåŸºç¡€å‚æ•°ï¼‰...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        print("âœ… åŸºç¡€tokenizeråŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŸºç¡€tokenizeråŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        
    # æµ‹è¯•4ï¼šåŠ è½½tokenizerï¼ˆæ·»åŠ trust_remote_codeï¼‰
    try:
        print("æµ‹è¯•4: åŠ è½½tokenizerï¼ˆtrust_remote_code=Trueï¼‰...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… å®Œæ•´tokenizeråŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å®Œæ•´tokenizeråŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return
    
    # æµ‹è¯•5ï¼šåŠ è½½æ¨¡å‹é…ç½®
    try:
        print("æµ‹è¯•5: åŠ è½½æ¨¡å‹é…ç½®...")
        from transformers import AutoConfig
        config = AutoConfig.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
        print(f"é…ç½®ä¿¡æ¯: {config}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹é…ç½®åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return
    
    # æµ‹è¯•6ï¼šåŠ è½½æ¨¡å‹ï¼ˆæœ€å°å‚æ•°ï¼‰
    try:
        print("æµ‹è¯•6: åŠ è½½æ¨¡å‹ï¼ˆæœ€å°å‚æ•°ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(model_path)
        print("âœ… æœ€å°å‚æ•°æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æœ€å°å‚æ•°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
    
    # æµ‹è¯•7ï¼šåŠ è½½æ¨¡å‹ï¼ˆé€æ­¥æ·»åŠ å‚æ•°ï¼‰
    try:
        print("æµ‹è¯•7: åŠ è½½æ¨¡å‹ï¼ˆtrust_remote_code=Trueï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… trust_remote_codeæ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ trust_remote_codeæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
    
    # æµ‹è¯•8ï¼šåŠ è½½æ¨¡å‹ï¼ˆæ·»åŠ torch_dtypeï¼‰
    try:
        print("æµ‹è¯•8: åŠ è½½æ¨¡å‹ï¼ˆtorch_dtype=torch.float16ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        print("âœ… float16æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ float16æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
    
    # æµ‹è¯•9ï¼šåŠ è½½æ¨¡å‹ï¼ˆå®Œæ•´å‚æ•°ï¼‰
    try:
        print("æµ‹è¯•9: åŠ è½½æ¨¡å‹ï¼ˆå®Œæ•´å‚æ•°ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… å®Œæ•´å‚æ•°æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å®Œæ•´å‚æ•°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ” è¿™é‡Œå¯èƒ½å°±æ˜¯é—®é¢˜æ‰€åœ¨ï¼")
        traceback.print_exc()

def test_with_different_parameters():
    """æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ"""
    print("\nğŸ§ª æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ...")
    
    model_path = Config.LLM_MODEL_PATH
    
    parameter_sets = [
        {
            "name": "é»˜è®¤å‚æ•°",
            "params": {}
        },
        {
            "name": "åªæ·»åŠ trust_remote_code",
            "params": {"trust_remote_code": True}
        },
        {
            "name": "æ·»åŠ torch_dtype",
            "params": {
                "torch_dtype": torch.float16,
                "trust_remote_code": True
            }
        },
        {
            "name": "ä½¿ç”¨CPUè®¾å¤‡æ˜ å°„",
            "params": {
                "torch_dtype": torch.float16,
                "device_map": "cpu",
                "trust_remote_code": True
            }
        },
        {
            "name": "ä½¿ç”¨cudaè®¾å¤‡æ˜ å°„",
            "params": {
                "torch_dtype": torch.float16,
                "device_map": "cuda:0",
                "trust_remote_code": True
            }
        },
        {
            "name": "åŸå§‹autoè®¾å¤‡æ˜ å°„",
            "params": {
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "trust_remote_code": True
            }
        }
    ]
    
    for param_set in parameter_sets:
        print(f"\næµ‹è¯•: {param_set['name']}")
        print(f"å‚æ•°: {param_set['params']}")
        
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **param_set['params']
            )
            print(f"âœ… {param_set['name']} - æˆåŠŸ")
            del model  # é‡Šæ”¾å†…å­˜
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âŒ {param_set['name']} - å¤±è´¥: {e}")
            if "NoneType" in str(e):
                print("ğŸ¯ å‘ç°NoneTypeé”™è¯¯ï¼")
                traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ› LLMåŠ è½½é—®é¢˜ç²¾ç¡®è°ƒè¯•")
    print("="*60)
    
    # æµ‹è¯•1ï¼šä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä»£ç 
    success, model, tokenizer = test_exact_same_code_as_rag()
    if success:
        print("ğŸ‰ é—®é¢˜è§£å†³ï¼ä½¿ç”¨ç›¸åŒä»£ç å¯ä»¥æ­£å¸¸åŠ è½½")
        return
    
    # æµ‹è¯•2ï¼šé€æ­¥æµ‹è¯•
    test_step_by_step()
    
    # æµ‹è¯•3ï¼šä¸åŒå‚æ•°ç»„åˆ
    test_with_different_parameters()
    
    print("\n" + "="*60)
    print("ğŸ” è°ƒè¯•æ€»ç»“")
    print("="*60)
    print("è¯·æŸ¥çœ‹ä¸Šé¢çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œæ‰¾å‡ºå¯¼è‡´ 'NoneType' é”™è¯¯çš„å…·ä½“å‚æ•°")

if __name__ == "__main__":
    main() 