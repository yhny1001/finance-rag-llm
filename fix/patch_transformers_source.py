#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç›´æ¥ä¿®è¡¥transformersæºç æ–‡ä»¶æ¥è§£å†³ALL_PARALLEL_STYLESé—®é¢˜
"""

import os
import sys
import shutil
from pathlib import Path

def find_transformers_path():
    """æ‰¾åˆ°transformerså®‰è£…è·¯å¾„"""
    try:
        import transformers
        transformers_path = Path(transformers.__file__).parent
        print(f"ğŸ“ transformerså®‰è£…è·¯å¾„: {transformers_path}")
        return transformers_path
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥transformers")
        return None

def patch_modeling_utils():
    """ä¿®è¡¥modeling_utils.pyæ–‡ä»¶"""
    print("ğŸ”§ ä¿®è¡¥modeling_utils.py...")
    
    try:
        transformers_path = find_transformers_path()
        if not transformers_path:
            return False
        
        modeling_utils_file = transformers_path / "modeling_utils.py"
        if not modeling_utils_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {modeling_utils_file}")
            return False
        
        print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {modeling_utils_file}")
        
        # å¤‡ä»½åŸæ–‡ä»¶
        backup_file = modeling_utils_file.with_suffix('.py.backup')
        if not backup_file.exists():
            shutil.copy2(modeling_utils_file, backup_file)
            print(f"ğŸ“¦ å·²å¤‡ä»½åˆ°: {backup_file}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(modeling_utils_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾é—®é¢˜ä»£ç è¡Œ
        problem_line = "if v not in ALL_PARALLEL_STYLES:"
        if problem_line in content:
            print("ğŸ¯ æ‰¾åˆ°é—®é¢˜ä»£ç è¡Œ")
            
            # æ›¿æ¢ä¸ºå®‰å…¨çš„æ£€æŸ¥
            safe_check = "if ALL_PARALLEL_STYLES is not None and v not in ALL_PARALLEL_STYLES:"
            content = content.replace(problem_line, safe_check)
            print("âœ… å·²æ›¿æ¢ä¸ºå®‰å…¨æ£€æŸ¥")
            
            # æ£€æŸ¥ALL_PARALLEL_STYLESå®šä¹‰
            if "ALL_PARALLEL_STYLES = None" in content:
                print("ğŸ”§ ä¿®å¤ALL_PARALLEL_STYLESå®šä¹‰...")
                content = content.replace(
                    "ALL_PARALLEL_STYLES = None",
                    'ALL_PARALLEL_STYLES = ["fsdp", "deepspeed", "megatron", "torchpaxus"]'
                )
                print("âœ… å·²ä¿®å¤ALL_PARALLEL_STYLESå®šä¹‰")
            
            # å†™å›æ–‡ä»¶
            with open(modeling_utils_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print("âœ… æ–‡ä»¶ä¿®è¡¥å®Œæˆ")
            return True
        else:
            print("âš ï¸ æœªæ‰¾åˆ°é—®é¢˜ä»£ç è¡Œ")
            return False
            
    except Exception as e:
        print(f"âŒ ä¿®è¡¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def patch_qwen3_modeling():
    """ä¿®è¡¥qwen3æ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ”§ ä¿®è¡¥qwen3 modelingæ–‡ä»¶...")
    
    try:
        transformers_path = find_transformers_path()
        if not transformers_path:
            return False
        
        qwen3_dir = transformers_path / "models" / "qwen3"
        modeling_qwen3_file = qwen3_dir / "modeling_qwen3.py"
        
        if not modeling_qwen3_file.exists():
            print(f"âŒ Qwen3æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {modeling_qwen3_file}")
            return False
        
        print(f"ğŸ“ Qwen3æ¨¡å‹æ–‡ä»¶: {modeling_qwen3_file}")
        
        # å¤‡ä»½
        backup_file = modeling_qwen3_file.with_suffix('.py.backup')
        if not backup_file.exists():
            shutil.copy2(modeling_qwen3_file, backup_file)
            print(f"ğŸ“¦ å·²å¤‡ä»½åˆ°: {backup_file}")
        
        # è¯»å–å†…å®¹
        with open(modeling_qwen3_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾post_initè°ƒç”¨å¹¶æ·»åŠ ä¿æŠ¤
        if "self.post_init()" in content:
            print("ğŸ¯ æ‰¾åˆ°post_init()è°ƒç”¨")
            
            # æ›¿æ¢ä¸ºå—ä¿æŠ¤çš„è°ƒç”¨
            protected_call = """
        try:
            self.post_init()
        except (TypeError, AttributeError) as e:
            if "argument of type 'NoneType' is not iterable" in str(e):
                print("âš ï¸ è·³è¿‡post_initæ£€æŸ¥ï¼ˆALL_PARALLEL_STYLESé—®é¢˜ï¼‰")
                pass
            else:
                raise e"""
            
            content = content.replace("        self.post_init()", protected_call)
            print("âœ… å·²æ·»åŠ post_initä¿æŠ¤")
            
            # å†™å›æ–‡ä»¶
            with open(modeling_qwen3_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print("âœ… Qwen3æ¨¡å‹æ–‡ä»¶ä¿®è¡¥å®Œæˆ")
            return True
        else:
            print("âš ï¸ æœªæ‰¾åˆ°post_init()è°ƒç”¨")
            return False
            
    except Exception as e:
        print(f"âŒ Qwen3ä¿®è¡¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_patched_loading():
    """æµ‹è¯•ä¿®è¡¥åçš„åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•ä¿®è¡¥åçš„æ¨¡å‹åŠ è½½...")
    
    try:
        # é‡æ–°å¯¼å…¥transformersä»¥ä½¿ç”¨ä¿®è¡¥åçš„ä»£ç 
        import importlib
        import sys
        
        # æ¸…é™¤ç¼“å­˜çš„æ¨¡å—
        modules_to_reload = [m for m in sys.modules.keys() if m.startswith('transformers')]
        for module in modules_to_reload:
            if module in sys.modules:
                del sys.modules[module]
        
        print("ğŸ”„ é‡æ–°å¯¼å…¥transformers...")
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        from config import Config
        model_path = Config.LLM_MODEL_PATH
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        
        # æµ‹è¯•tokenizer
        print("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… tokenizeråŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # ç®€å•æµ‹è¯•
        print("ç®€å•ç”Ÿæˆæµ‹è¯•...")
        test_text = "é“¶è¡Œèµ„æœ¬å……è¶³ç‡æ˜¯ä»€ä¹ˆï¼Ÿ"
        inputs = tokenizer(test_text, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)
        print(f"ç”Ÿæˆç»“æœ: {response}")
        
        # æ¸…ç†
        del model, tokenizer, outputs, inputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("âœ… ä¿®è¡¥åæµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®è¡¥åæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def restore_files():
    """æ¢å¤åŸå§‹æ–‡ä»¶"""
    print("\nğŸ”„ æ¢å¤è¯´æ˜...")
    
    try:
        transformers_path = find_transformers_path()
        if transformers_path:
            modeling_utils_backup = transformers_path / "modeling_utils.py.backup"
            qwen3_backup = transformers_path / "models" / "qwen3" / "modeling_qwen3.py.backup"
            
            print("ğŸ’¡ å¦‚æœéœ€è¦æ¢å¤åŸå§‹æ–‡ä»¶ï¼Œè¯·è¿è¡Œï¼š")
            if modeling_utils_backup.exists():
                print(f"   cp {modeling_utils_backup} {transformers_path}/modeling_utils.py")
            if qwen3_backup.exists():
                print(f"   cp {qwen3_backup} {transformers_path}/models/qwen3/modeling_qwen3.py")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¢å¤è¯´æ˜å¤±è´¥: {e}")
        return False

def main():
    """ä¸»ä¿®è¡¥æµç¨‹"""
    print("ğŸš‘ ç›´æ¥ä¿®è¡¥transformersæºç ")
    print("=" * 50)
    
    steps = [
        ("ä¿®è¡¥modeling_utils.py", patch_modeling_utils),
        ("ä¿®è¡¥qwen3æ¨¡å‹æ–‡ä»¶", patch_qwen3_modeling),
        ("æµ‹è¯•ä¿®è¡¥æ•ˆæœ", test_patched_loading)
    ]
    
    success_count = 0
    for step_name, step_func in steps:
        print(f"\nğŸ”§ {step_name}...")
        if step_func():
            print(f"âœ… {step_name}æˆåŠŸ")
            success_count += 1
        else:
            print(f"âŒ {step_name}å¤±è´¥")
            if step_name == "æµ‹è¯•ä¿®è¡¥æ•ˆæœ":
                break  # å¦‚æœæµ‹è¯•å¤±è´¥ï¼Œä¸ç»§ç»­
    
    if success_count >= 2:  # è‡³å°‘ä¿®è¡¥æˆåŠŸ
        print("\nğŸ‰ æºç ä¿®è¡¥å®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œ99_quick_verify.py")
        restore_files()
        return True
    else:
        print("\nâŒ æºç ä¿®è¡¥å¤±è´¥")
        restore_files()
        return False

if __name__ == "__main__":
    main() 