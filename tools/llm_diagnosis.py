#!/usr/bin/env python
"""
LLMæ¨¡å‹åŠ è½½è¯Šæ–­è„šæœ¬
å¸®åŠ©æ’æŸ¥Qwen2.5-7B-Instructæ¨¡å‹åŠ è½½é—®é¢˜
"""

import os
import torch
from pathlib import Path
from config import Config

def check_model_path():
    """æ£€æŸ¥æ¨¡å‹è·¯å¾„"""
    print("ğŸ” æ£€æŸ¥æ¨¡å‹è·¯å¾„...")
    model_path = Config.LLM_MODEL_PATH
    print(f"LLMæ¨¡å‹è·¯å¾„: {model_path}")
    
    if not Path(model_path).exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    print("âœ… æ¨¡å‹è·¯å¾„å­˜åœ¨")
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    key_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json']
    for file in key_files:
        file_path = Path(model_path) / file
        if file_path.exists():
            print(f"âœ… {file} å­˜åœ¨")
        else:
            print(f"âš ï¸  {file} ä¸å­˜åœ¨")
    
    return True

def check_transformers_version():
    """æ£€æŸ¥transformersç‰ˆæœ¬"""
    print("\nğŸ” æ£€æŸ¥transformersç‰ˆæœ¬...")
    try:
        import transformers
        print(f"transformersç‰ˆæœ¬: {transformers.__version__}")
        
        # æ¨èç‰ˆæœ¬
        import packaging.version as pv
        current_version = pv.parse(transformers.__version__)
        min_version = pv.parse("4.20.0")
        
        if current_version >= min_version:
            print("âœ… transformersç‰ˆæœ¬ç¬¦åˆè¦æ±‚")
        else:
            print(f"âš ï¸  transformersç‰ˆæœ¬è¾ƒä½ï¼Œæ¨èå‡çº§åˆ°4.20.0+")
        
        return True
    except ImportError:
        print("âŒ transformersæœªå®‰è£…")
        return False

def test_simple_tokenizer_load():
    """æµ‹è¯•ç®€å•çš„tokenizeråŠ è½½"""
    print("\nğŸ” æµ‹è¯•tokenizeråŠ è½½...")
    try:
        from transformers import AutoTokenizer
        
        model_path = Config.LLM_MODEL_PATH
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… tokenizeråŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç¼–ç 
        test_text = "ä½ å¥½"
        tokens = tokenizer.encode(test_text)
        print(f"âœ… tokenizerç¼–ç æµ‹è¯•æˆåŠŸ: {test_text} -> {tokens}")
        
        return True
    except Exception as e:
        print(f"âŒ tokenizeråŠ è½½å¤±è´¥: {e}")
        return False

def test_model_config():
    """æµ‹è¯•æ¨¡å‹é…ç½®åŠ è½½"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹é…ç½®åŠ è½½...")
    try:
        from transformers import AutoConfig
        
        model_path = Config.LLM_MODEL_PATH
        config = AutoConfig.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹ç±»å‹: {config.model_type}")
        print(f"æ¨¡å‹æ¶æ„: {config.architectures}")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜"""
    print("\nğŸ” æ£€æŸ¥GPUçŠ¶æ€...")
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        return False
    
    print(f"âœ… CUDAå¯ç”¨ï¼Œç‰ˆæœ¬: {torch.version.cuda}")
    
    for i in range(torch.cuda.device_count()):
        gpu_name = torch.cuda.get_device_name(i)
        memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
        memory_free = memory_total - memory_allocated
        
        print(f"GPU {i}: {gpu_name}")
        print(f"  æ€»å†…å­˜: {memory_total:.2f} GB")
        print(f"  å·²ç”¨å†…å­˜: {memory_allocated:.2f} GB")
        print(f"  å¯ç”¨å†…å­˜: {memory_free:.2f} GB")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜åŠ è½½7Bæ¨¡å‹
        if memory_free < 14:  # 7Bæ¨¡å‹å¤§çº¦éœ€è¦14GB
            print(f"  âš ï¸  å¯ç”¨å†…å­˜ä¸è¶³ï¼Œ7Bæ¨¡å‹éœ€è¦çº¦14GB")
        else:
            print(f"  âœ… å†…å­˜å……è¶³")
    
    return True

def suggest_solutions():
    """æä¾›è§£å†³æ–¹æ¡ˆå»ºè®®"""
    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
    print("1. å¦‚æœæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®ä¸‹è½½")
    print("2. å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥å°è¯•:")
    print("   - ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚Qwen2.5-3Bï¼‰")
    print("   - å¯ç”¨8bitæˆ–4bité‡åŒ–")
    print("   - è°ƒæ•´device_mapå‚æ•°")
    print("3. å¦‚æœtransformersç‰ˆæœ¬é—®é¢˜ï¼Œè¯·å‡çº§:")
    print("   pip install -U transformers")
    print("4. æš‚æ—¶çš„æ›¿ä»£æ–¹æ¡ˆ:")
    print("   - ç³»ç»Ÿå¯ä»¥ä»…ä½¿ç”¨å‘é‡æ£€ç´¢åŠŸèƒ½")
    print("   - ä½¿ç”¨æ›´å°çš„æœ¬åœ°æ¨¡å‹")
    print("   - é›†æˆåœ¨çº¿APIæœåŠ¡")

def main():
    """ä¸»è¯Šæ–­å‡½æ•°"""
    print("="*60)
    print("ğŸ”§ LLMæ¨¡å‹åŠ è½½è¯Šæ–­å·¥å…·")
    print("="*60)
    
    checks = [
        ("æ¨¡å‹è·¯å¾„æ£€æŸ¥", check_model_path),
        ("transformersç‰ˆæœ¬æ£€æŸ¥", check_transformers_version),
        ("tokenizeråŠ è½½æµ‹è¯•", test_simple_tokenizer_load),
        ("æ¨¡å‹é…ç½®æµ‹è¯•", test_model_config),
        ("GPUå†…å­˜æ£€æŸ¥", check_gpu_memory),
    ]
    
    results = []
    for check_name, check_func in checks:
        print(f"\n{'='*20} {check_name} {'='*20}")
        try:
            result = check_func()
            results.append((check_name, result))
        except Exception as e:
            print(f"âŒ {check_name}å¤±è´¥: {e}")
            results.append((check_name, False))
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ” è¯Šæ–­ç»“æœæ€»ç»“")
    print("="*60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for check_name, result in results:
        status = "âœ…" if result else "âŒ"
        print(f"{status} {check_name}")
    
    print(f"\né€šè¿‡: {passed}/{total}")
    
    # æä¾›å»ºè®®
    suggest_solutions()
    
    # ç³»ç»ŸçŠ¶æ€è¯„ä¼°
    print("\n" + "="*60)
    print("ğŸ“Š ç³»ç»ŸçŠ¶æ€è¯„ä¼°")
    print("="*60)
    
    if passed >= 4:
        print("ğŸŸ¢ ç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼ŒLLMé—®é¢˜å¯èƒ½æ˜¯æš‚æ—¶çš„é…ç½®é—®é¢˜")
    elif passed >= 2:
        print("ğŸŸ¡ ç³»ç»Ÿéƒ¨åˆ†åŠŸèƒ½æ­£å¸¸ï¼Œå‘é‡æ£€ç´¢å¯ä»¥ä½¿ç”¨")
    else:
        print("ğŸ”´ ç³»ç»Ÿå­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥ç¯å¢ƒé…ç½®")
    
    print("\nâœ¨ å¥½æ¶ˆæ¯ï¼šå³ä½¿LLMåŠ è½½å¤±è´¥ï¼Œå‘é‡æ£€ç´¢åŠŸèƒ½ä»ç„¶å¯ä»¥æ­£å¸¸å·¥ä½œï¼")

if __name__ == "__main__":
    main() 