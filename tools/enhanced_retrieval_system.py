#!/usr/bin/env python3
"""
å¢å¼ºæ£€ç´¢ç³»ç»Ÿ - è§£å†³æ£€ç´¢ç›¸å…³æ€§ä½çš„é—®é¢˜
åŒ…å«å¤šç§æ£€ç´¢ç­–ç•¥ï¼šæ··åˆæ£€ç´¢ã€æŸ¥è¯¢æ‰©å±•ã€é‡æ’åºç­‰
"""

import os
import re
# å°è¯•å¯¼å…¥jiebaï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€å•åˆ†è¯
try:
    import jieba
    JIEBA_AVAILABLE = True
    print("âœ… jiebaå¯ç”¨")
except ImportError:
    JIEBA_AVAILABLE = False
    print("âš ï¸ jiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†è¯")

# å¯¼å…¥reæ¨¡å— - ä¿®å¤å˜é‡ä½œç”¨åŸŸé—®é¢˜
import re
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple
from collections import Counter, defaultdict
import json
from dataclasses import dataclass

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç»“æ„"""
    text: str
    score: float
    source: str  # 'vector', 'keyword', 'hybrid'
    metadata: Dict[str, Any]
    chunk_id: str
    vector_score: float = 0.0
    keyword_score: float = 0.0
    rerank_score: float = 0.0

class QueryProcessor:
    """æŸ¥è¯¢å¤„ç†å™¨ - æ‰©å±•å’Œä¼˜åŒ–æŸ¥è¯¢"""
    
    def __init__(self):
        # é‡‘èé¢†åŸŸåŒä¹‰è¯æ˜ å°„
        self.financial_synonyms = {
            'é“¶è¡Œ': ['é‡‘èæœºæ„', 'é“¶è¡Œä¸š', 'é“¶è¡Œä¸šæœºæ„', 'å•†ä¸šé“¶è¡Œ', 'æ”¿ç­–æ€§é“¶è¡Œ'],
            'ç›‘ç®¡': ['ç®¡ç†', 'ç›‘ç£', 'è§„åˆ¶', 'æ²»ç†', 'ç®¡æ§'],
            'èµ„æœ¬': ['èµ„é‡‘', 'èµ„æœ¬é‡‘', 'è‡ªæœ‰èµ„é‡‘', 'è‚¡æœ¬'],
            'é£é™©': ['é£é™©ç®¡ç†', 'é£é™©æ§åˆ¶', 'é£é™©é˜²èŒƒ', 'é£é™©è¯„ä¼°'],
            'å­˜æ¬¾': ['å‚¨è“„', 'å­˜å‚¨', 'èµ„é‡‘å­˜æ”¾'],
            'è´·æ¬¾': ['æ”¾è´·', 'ä¿¡è´·', 'èèµ„', 'å€Ÿè´·'],
            'åˆ©ç‡': ['åˆ©æ¯ç‡', 'æ¯ç‡', 'èµ„é‡‘æˆæœ¬'],
            'ä¿é™©': ['ä¿é™©ä¸š', 'ä¿é™©æœºæ„', 'ä¿é™©å…¬å¸'],
            'è¯åˆ¸': ['è¯åˆ¸ä¸š', 'è¯åˆ¸å¸‚åœº', 'èµ„æœ¬å¸‚åœº'],
            'åŸºé‡‘': ['æŠ•èµ„åŸºé‡‘', 'å…¬å‹ŸåŸºé‡‘', 'ç§å‹ŸåŸºé‡‘'],
            'å¤®è¡Œ': ['ä¸­å›½äººæ°‘é“¶è¡Œ', 'äººæ°‘é“¶è¡Œ', 'ä¸­å¤®é“¶è¡Œ'],
            'é“¶ç›‘ä¼š': ['é“¶è¡Œä¸šç›‘ç£ç®¡ç†å§”å‘˜ä¼š', 'é“¶ä¿ç›‘ä¼š', 'ç›‘ç®¡éƒ¨é—¨'],
            'èµ„æœ¬å……è¶³ç‡': ['èµ„æœ¬å……è¶³æ€§', 'èµ„æœ¬æ¯”ç‡', 'èµ„æœ¬æ°´å¹³'],
            'æµåŠ¨æ€§': ['èµ„é‡‘æµåŠ¨æ€§', 'æµåŠ¨æ€§ç®¡ç†', 'æµåŠ¨æ€§é£é™©'],
            'åˆè§„': ['åˆè§„æ€§', 'åˆæ³•åˆè§„', 'è§„èŒƒæ€§'],
            'å†…æ§': ['å†…éƒ¨æ§åˆ¶', 'å†…æ§åˆ¶åº¦', 'å†…æ§ç®¡ç†'],
            'å®¡è®¡': ['å®¡è®¡ç›‘ç£', 'å†…éƒ¨å®¡è®¡', 'å¤–éƒ¨å®¡è®¡'],
            'åæ´—é’±': ['AML', 'æ´—é’±é˜²èŒƒ', 'åæ´—é’±åˆè§„'],
            'å¾ä¿¡': ['ä¿¡ç”¨è®°å½•', 'ä¿¡ç”¨ä¿¡æ¯', 'å¾ä¿¡ç³»ç»Ÿ'],
            'ä¸è‰¯èµ„äº§': ['ä¸è‰¯è´·æ¬¾', 'åè´¦', 'é—®é¢˜èµ„äº§']
        }
        
        # å…³é”®æœ¯è¯­æƒé‡
        self.term_weights = {
            'é“¶è¡Œ': 1.5, 'ç›‘ç®¡': 1.4, 'èµ„æœ¬': 1.3, 'é£é™©': 1.3,
            'å­˜æ¬¾': 1.2, 'è´·æ¬¾': 1.2, 'ä¿é™©': 1.2, 'åˆ©ç‡': 1.1,
            'å¤®è¡Œ': 1.4, 'é“¶ç›‘ä¼š': 1.4, 'åˆè§„': 1.3, 'å®¡è®¡': 1.2
        }
        
        # å¸¸è§é—®é¢˜ç±»å‹æ¨¡æ¿
        self.query_patterns = {
            'definition': ['æ˜¯ä»€ä¹ˆ', 'å®šä¹‰', 'å«ä¹‰', 'æ¦‚å¿µ'],
            'procedure': ['å¦‚ä½•', 'æ€æ ·', 'ç¨‹åº', 'æ­¥éª¤', 'æµç¨‹'],
            'requirement': ['è¦æ±‚', 'æ¡ä»¶', 'æ ‡å‡†', 'è§„å®š'],
            'calculation': ['è®¡ç®—', 'å…¬å¼', 'æ¯”ä¾‹', 'æ¯”ç‡'],
            'regulation': ['æ³•è§„', 'è§„å®š', 'åŠæ³•', 'é€šçŸ¥', 'æŒ‡å¯¼æ„è§']
        }
    
    def expand_query(self, query: str) -> Dict[str, Any]:
        """æ‰©å±•æŸ¥è¯¢ï¼Œå¢åŠ åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­"""
        # åˆ†è¯
        if JIEBA_AVAILABLE:
            words = list(jieba.cut(query))
        else:
            # ç®€å•åˆ†è¯ - æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
            words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+', query)
        
        # åŸå§‹æŸ¥è¯¢è¯
        original_terms = set(words)
        
        # æ‰©å±•è¯æ±‡
        expanded_terms = set(words)
        important_terms = []
        
        # æ·»åŠ åŒä¹‰è¯
        for word in words:
            if word in self.financial_synonyms:
                expanded_terms.update(self.financial_synonyms[word])
                important_terms.append(word)
        
        # è¯†åˆ«æŸ¥è¯¢ç±»å‹
        query_type = self._identify_query_type(query)
        
        # æå–å…³é”®æ•°å­—å’Œæ¯”ä¾‹
        numbers = re.findall(r'\d+(?:\.\d+)?%?', query)
        
        # æå–æ³•è§„ç›¸å…³è¯æ±‡
        regulation_terms = []
        reg_patterns = ['æ¡', 'æ¬¾', 'é¡¹', 'ç¬¬.*æ¡', 'é“¶å‘', 'é“¶ç›‘å‘', 'ä¿ç›‘å‘']
        for pattern in reg_patterns:
            if re.search(pattern, query):
                regulation_terms.append(pattern)
        
        return {
            'original_query': query,
            'original_terms': list(original_terms),
            'expanded_terms': list(expanded_terms),
            'important_terms': important_terms,
            'query_type': query_type,
            'numbers': numbers,
            'regulation_terms': regulation_terms,
            'weights': {term: self.term_weights.get(term, 1.0) for term in expanded_terms}
        }
    
    def _identify_query_type(self, query: str) -> str:
        """è¯†åˆ«æŸ¥è¯¢ç±»å‹"""
        for qtype, patterns in self.query_patterns.items():
            if any(pattern in query for pattern in patterns):
                return qtype
        return 'general'
    
    def generate_search_variants(self, query: str) -> List[str]:
        """ç”ŸæˆæŸ¥è¯¢å˜ä½“"""
        expanded = self.expand_query(query)
        variants = [query]  # åŸå§‹æŸ¥è¯¢
        
        # æ·»åŠ é‡è¦æœ¯è¯­ç»„åˆ
        if expanded['important_terms']:
            for term in expanded['important_terms']:
                if term in expanded['expanded_terms']:
                    # ç”¨åŒä¹‰è¯æ›¿æ¢
                    for synonym in self.financial_synonyms.get(term, []):
                        variant = query.replace(term, synonym)
                        if variant != query:
                            variants.append(variant)
        
        # æ·»åŠ ç®€åŒ–æŸ¥è¯¢ï¼ˆåªä¿ç•™å…³é”®è¯ï¼‰
        if len(expanded['important_terms']) > 0:
            simplified = ' '.join(expanded['important_terms'])
            if simplified != query:
                variants.append(simplified)
        
        return variants[:5]  # é™åˆ¶å˜ä½“æ•°é‡

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ - ç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢"""
    
    def __init__(self, vector_db, query_processor):
        self.vector_db = vector_db
        self.query_processor = query_processor
        self.document_store = vector_db.document_store if hasattr(vector_db, 'document_store') else {}
        self._build_keyword_index()
    
    def _build_keyword_index(self):
        """æ„å»ºå…³é”®è¯å€’æ’ç´¢å¼•"""
        print("ğŸ”§ æ„å»ºå…³é”®è¯ç´¢å¼•...")
        self.keyword_index = defaultdict(list)
        self.document_keywords = {}
        
        if not self.document_store:
            print("âš ï¸ æ–‡æ¡£å­˜å‚¨ä¸ºç©ºï¼Œè·³è¿‡å…³é”®è¯ç´¢å¼•æ„å»º")
            return
        
        processed_docs = 0
        for chunk_id, doc_data in self.document_store.items():
            try:
                text = doc_data.get('text', '')
                if not text or len(text.strip()) < 10:
                    continue
                    
                # åˆ†è¯å¹¶å»ºç«‹ç´¢å¼•
                if JIEBA_AVAILABLE:
                    words = list(jieba.cut(text.lower()))
                else:
                    words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+', text.lower())
                
                # è¿‡æ»¤å¤ªçŸ­çš„è¯
                words = [w for w in words if len(w) > 1]
                if not words:
                    continue
                    
                unique_words = set(words)
                self.document_keywords[chunk_id] = words
                processed_docs += 1
                
                # å»ºç«‹å€’æ’ç´¢å¼•
                for word in unique_words:
                    word_count = words.count(word)
                    tf = word_count / len(words) if words else 0  # è¯é¢‘
                    self.keyword_index[word].append({
                        'chunk_id': chunk_id,
                        'tf': tf,
                        'text': text
                    })
                    
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡æ¡£ {chunk_id} æ—¶å‡ºé”™: {e}")
                continue
        
        if processed_docs == 0:
            print("âŒ æ²¡æœ‰å¤„ç†ä»»ä½•æ–‡æ¡£ï¼Œå…³é”®è¯ç´¢å¼•ä¸ºç©º")
            return
        
        # è®¡ç®—IDF
        total_docs = len(self.document_store)
        self.idf_scores = {}
        for word, docs in self.keyword_index.items():
            df = len(docs)  # æ–‡æ¡£é¢‘ç‡
            if df > 0:
                self.idf_scores[word] = np.log(total_docs / (df + 1))
        
        print(f"âœ… å…³é”®è¯ç´¢å¼•æ„å»ºå®Œæˆ")
        print(f"   å¤„ç†æ–‡æ¡£: {processed_docs}/{len(self.document_store)}")
        print(f"   è¯æ±‡æ•°é‡: {len(self.keyword_index)}")
        print(f"   IDFåˆ†æ•°: {len(self.idf_scores)}")
    
    def keyword_search(self, query: str, top_k: int = 20) -> List[RetrievalResult]:
        """å…³é”®è¯æ£€ç´¢"""
        try:
            if not self.keyword_index:
                print("âš ï¸ å…³é”®è¯ç´¢å¼•ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
                return []
            
            expanded = self.query_processor.expand_query(query)
            query_terms = expanded['expanded_terms']
            term_weights = expanded['weights']
            
            print(f"ğŸ” å…³é”®è¯æ£€ç´¢æŸ¥è¯¢è¯: {query_terms[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
            
            # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„BM25åˆ†æ•°
            doc_scores = defaultdict(float)
            matched_terms = 0
            
            for term in query_terms:
                term_lower = term.lower()
                if term_lower not in self.keyword_index:
                    continue
                    
                matched_terms += 1
                idf = self.idf_scores.get(term_lower, 0)
                weight = term_weights.get(term, 1.0)
                
                for doc_info in self.keyword_index[term_lower]:
                    chunk_id = doc_info['chunk_id']
                    tf = doc_info['tf']
                    
                    # BM25è®¡ç®— (ç®€åŒ–ç‰ˆ)
                    k1, b = 1.5, 0.75
                    doc_len = len(self.document_keywords.get(chunk_id, []))
                    
                    # å®‰å…¨çš„å¹³å‡æ–‡æ¡£é•¿åº¦è®¡ç®—
                    if self.document_keywords:
                        avg_doc_len = np.mean([len(words) for words in self.document_keywords.values()])
                    else:
                        avg_doc_len = 1
                    
                    if avg_doc_len == 0:
                        avg_doc_len = 1
                        
                    score = idf * weight * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))
                    doc_scores[chunk_id] += score
            
            print(f"ğŸ“Š åŒ¹é…æŸ¥è¯¢è¯æ•°: {matched_terms}/{len(query_terms)}")
            print(f"ğŸ“Š å€™é€‰æ–‡æ¡£æ•°: {len(doc_scores)}")
            
            if not doc_scores:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
                return []
            
            # æ’åºå¹¶è¿”å›ç»“æœ
            sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
            
            results = []
            for chunk_id, score in sorted_docs[:top_k]:
                if chunk_id in self.document_store:
                    doc_data = self.document_store[chunk_id]
                    result = RetrievalResult(
                        text=doc_data.get('text', ''),
                        score=score,
                        source='keyword',
                        metadata=doc_data.get('doc_metadata', {}),
                        chunk_id=chunk_id,
                        keyword_score=score
                    )
                    results.append(result)
            
            print(f"âœ… å…³é”®è¯æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def vector_search(self, query: str, top_k: int = 20) -> List[RetrievalResult]:
        """å‘é‡æ£€ç´¢"""
        try:
            vector_results = self.vector_db.search(query, top_k=top_k)
            
            results = []
            for result in vector_results:
                retrieval_result = RetrievalResult(
                    text=result.get('text', ''),
                    score=result.get('score', 0.0),
                    source='vector',
                    metadata=result.get('metadata', {}),
                    chunk_id=result.get('chunk_id', ''),
                    vector_score=result.get('score', 0.0)
                )
                results.append(retrieval_result)
            
            return results
        except Exception as e:
            print(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def hybrid_search(self, query: str, top_k: int = 15) -> List[RetrievalResult]:
        """æ··åˆæ£€ç´¢ - ç»“åˆå‘é‡å’Œå…³é”®è¯æ£€ç´¢"""
        print(f"æ‰§è¡Œæ··åˆæ£€ç´¢: {query}")
        
        # ç”ŸæˆæŸ¥è¯¢å˜ä½“
        query_variants = self.query_processor.generate_search_variants(query)
        print(f"æŸ¥è¯¢å˜ä½“: {query_variants}")
        
        all_results = {}  # chunk_id -> RetrievalResult
        
        # 1. å‘é‡æ£€ç´¢
        vector_results = self.vector_search(query, top_k=top_k*2)
        for result in vector_results:
            if result.chunk_id not in all_results:
                all_results[result.chunk_id] = result
                all_results[result.chunk_id].source = 'vector'
        
        # 2. å…³é”®è¯æ£€ç´¢ï¼ˆåŸæŸ¥è¯¢ï¼‰
        keyword_results = self.keyword_search(query, top_k=top_k*2)
        for result in keyword_results:
            if result.chunk_id in all_results:
                # åˆå¹¶åˆ†æ•°
                existing = all_results[result.chunk_id]
                existing.keyword_score = result.keyword_score
                existing.score = existing.vector_score * 0.6 + result.keyword_score * 0.4
                existing.source = 'hybrid'
            else:
                all_results[result.chunk_id] = result
                result.source = 'keyword'
        
        # 3. æŸ¥è¯¢å˜ä½“æ£€ç´¢ï¼ˆåªåšå…³é”®è¯æ£€ç´¢ï¼‰
        for variant in query_variants[1:3]:  # åªç”¨å‰2ä¸ªå˜ä½“
            variant_results = self.keyword_search(variant, top_k=top_k)
            for result in variant_results:
                if result.chunk_id in all_results:
                    # æå‡å·²æœ‰ç»“æœçš„åˆ†æ•°
                    all_results[result.chunk_id].score += result.keyword_score * 0.2
                else:
                    result.score *= 0.8  # å˜ä½“ç»“æœé™æƒ
                    all_results[result.chunk_id] = result
        
        # æ’åºå¹¶è¿”å›
        final_results = list(all_results.values())
        final_results.sort(key=lambda x: x.score, reverse=True)
        
        print(f"æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} ä¸ªç»“æœ")
        return final_results[:top_k]

class RelevanceReranker:
    """ç›¸å…³æ€§é‡æ’åºå™¨"""
    
    def __init__(self):
        # é‡‘èæœ¯è¯­é‡è¦æ€§æƒé‡
        self.important_terms = {
            'é“¶è¡Œ': 2.0, 'ç›‘ç®¡': 1.8, 'èµ„æœ¬': 1.7, 'é£é™©': 1.7,
            'å­˜æ¬¾': 1.6, 'è´·æ¬¾': 1.6, 'ä¿é™©': 1.5, 'å¤®è¡Œ': 1.8,
            'åˆè§„': 1.6, 'å®¡è®¡': 1.4, 'åˆ©ç‡': 1.5, 'æµåŠ¨æ€§': 1.5,
            'èµ„æœ¬å……è¶³ç‡': 2.2, 'ä¸è‰¯èµ„äº§': 1.9, 'åæ´—é’±': 1.7
        }
        
        # æ–‡æ¡£è´¨é‡æŒ‡æ ‡
        self.quality_indicators = {
            'structure_markers': ['ç¬¬', 'æ¡', 'æ¬¾', 'é¡¹', 'ï¼ˆ', 'ï¼‰', '1.', '2.', 'ä¸€ã€', 'äºŒã€'],
            'regulation_markers': ['é“¶å‘', 'é“¶ç›‘å‘', 'ä¿ç›‘å‘', 'é€šçŸ¥', 'åŠæ³•', 'è§„å®š', 'æŒ‡å¯¼æ„è§'],
            'number_patterns': [r'\d+%', r'\d+\.\d+%', r'\d+å€', r'\d+ä¸‡', r'\d+äº¿'],
            'formal_language': ['åº”å½“', 'å¿…é¡»', 'ä¸å¾—', 'ç¦æ­¢', 'æŒ‰ç…§', 'æ ¹æ®', 'ä¾æ®']
        }
    
    def rerank_results(self, results: List[RetrievalResult], query: str) -> List[RetrievalResult]:
        """é‡æ–°æ’åºæ£€ç´¢ç»“æœ"""
        if not results:
            return results
        
        print(f"å¯¹ {len(results)} ä¸ªç»“æœè¿›è¡Œé‡æ’åº")
        
        # æ‰©å±•æŸ¥è¯¢
        if JIEBA_AVAILABLE:
            query_terms = set(jieba.cut(query.lower()))
        else:
            query_terms = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+', query.lower()))
        
        for result in results:
            # è®¡ç®—é‡æ’åºåˆ†æ•°
            relevance_score = self._calculate_relevance_score(result.text, query, query_terms)
            quality_score = self._calculate_quality_score(result.text)
            coverage_score = self._calculate_coverage_score(result.text, query_terms)
            
            # ç»¼åˆè¯„åˆ†
            result.rerank_score = (
                relevance_score * 0.4 +
                quality_score * 0.3 +
                coverage_score * 0.3
            )
            
            # æ›´æ–°æœ€ç»ˆåˆ†æ•°
            result.score = result.score * 0.6 + result.rerank_score * 0.4
        
        # é‡æ–°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        
        print("é‡æ’åºå®Œæˆ")
        return results
    
    def _calculate_relevance_score(self, text: str, query: str, query_terms: Set[str]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        text_lower = text.lower()
        if JIEBA_AVAILABLE:
            text_terms = set(jieba.cut(text_lower))
        else:
            text_terms = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+', text_lower))
        
        # åŸºç¡€åŒ¹é…åˆ†æ•°
        match_count = len(query_terms & text_terms)
        base_score = match_count / len(query_terms) if query_terms else 0
        
        # é‡è¦æœ¯è¯­åŠ æƒ
        weighted_score = 0
        for term in query_terms & text_terms:
            weight = self.important_terms.get(term, 1.0)
            weighted_score += weight
        
        # ä½ç½®æƒé‡ï¼ˆæŸ¥è¯¢è¯åœ¨æ–‡æ¡£å‰éƒ¨åˆ†æƒé‡æ›´é«˜ï¼‰
        position_score = 0
        for term in query_terms:
            pos = text_lower.find(term)
            if pos != -1:
                # å‰20%ä½ç½®æƒé‡1.5ï¼Œä¸­é—´60%æƒé‡1.0ï¼Œå20%æƒé‡0.8
                relative_pos = pos / len(text_lower)
                if relative_pos <= 0.2:
                    position_score += 1.5
                elif relative_pos <= 0.8:
                    position_score += 1.0
                else:
                    position_score += 0.8
        
        return (base_score + weighted_score * 0.3 + position_score * 0.2) / 3
    
    def _calculate_quality_score(self, text: str) -> float:
        """è®¡ç®—æ–‡æ¡£è´¨é‡åˆ†æ•°"""
        score = 0.0
        
        # ç»“æ„åŒ–ç¨‹åº¦
        structure_count = sum(1 for marker in self.quality_indicators['structure_markers'] 
                            if marker in text)
        score += min(structure_count * 0.1, 0.3)
        
        # æ³•è§„ç›¸å…³æ€§
        regulation_count = sum(1 for marker in self.quality_indicators['regulation_markers'] 
                             if marker in text)
        score += min(regulation_count * 0.15, 0.3)
        
        # æ•°å­—ä¿¡æ¯ä¸°å¯Œåº¦
        number_matches = sum(len(re.findall(pattern, text)) 
                           for pattern in self.quality_indicators['number_patterns'])
        score += min(number_matches * 0.1, 0.2)
        
        # æ­£å¼è¯­è¨€ä½¿ç”¨
        formal_count = sum(1 for phrase in self.quality_indicators['formal_language'] 
                          if phrase in text)
        score += min(formal_count * 0.05, 0.2)
        
        return min(score, 1.0)
    
    def _calculate_coverage_score(self, text: str, query_terms: Set[str]) -> float:
        """è®¡ç®—è¦†ç›–åº¦åˆ†æ•°"""
        if JIEBA_AVAILABLE:
            text_terms = set(jieba.cut(text.lower()))
        else:
            text_terms = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+', text.lower()))
        
        # ç›´æ¥åŒ¹é…
        direct_matches = len(query_terms & text_terms)
        
        # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
        partial_matches = 0
        for query_term in query_terms:
            for text_term in text_terms:
                if len(query_term) > 2 and query_term in text_term:
                    partial_matches += 0.5
                elif len(text_term) > 2 and text_term in query_term:
                    partial_matches += 0.3
        
        total_coverage = direct_matches + partial_matches
        return min(total_coverage / len(query_terms) if query_terms else 0, 1.0)

def integrate_enhanced_retrieval(rag_engine_path: str = "rag_engine.py"):
    """å°†å¢å¼ºæ£€ç´¢ç³»ç»Ÿé›†æˆåˆ°RAGå¼•æ“ä¸­"""
    print("ğŸ”§ æ£€æŸ¥å¢å¼ºæ£€ç´¢ç³»ç»Ÿé›†æˆçŠ¶æ€...")
    
    if not Path(rag_engine_path).exists():
        print(f"âŒ {rag_engine_path} æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # è¯»å–RAGå¼•æ“æ–‡ä»¶
    with open(rag_engine_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»é›†æˆ
    enhanced_indicators = [
        "ENHANCED_RETRIEVAL_AVAILABLE",
        "å¢å¼ºæ£€ç´¢æŸ¥è¯¢",
        "HybridRetriever",
        "QueryProcessor", 
        "RelevanceReranker"
    ]
    
    already_integrated = all(indicator in content for indicator in enhanced_indicators)
    
    if already_integrated:
        print("âœ… å¢å¼ºæ£€ç´¢ç³»ç»Ÿå·²ç»é›†æˆåˆ°RAGå¼•æ“ä¸­")
        
        # æ£€æŸ¥æ–¹æ³•æ˜¯å¦ä¸ºå¢å¼ºç‰ˆæœ¬
        if '"""æ£€ç´¢ç›¸å…³æ–‡æ¡£ - å¢å¼ºç‰ˆæœ¬"""' in content:
            print("âœ… retrieve_documentsæ–¹æ³•å·²å‡çº§ä¸ºå¢å¼ºç‰ˆæœ¬")
            return True
        else:
            print("âš ï¸ æ£€æµ‹åˆ°é›†æˆä½†æ–¹æ³•ç‰ˆæœ¬å¯èƒ½ä¸æ˜¯æœ€æ–°")
    
    # å¦‚æœæ²¡æœ‰å®Œå…¨é›†æˆï¼Œå°è¯•é›†æˆ
    print("ğŸ”„ æ‰§è¡Œå¢å¼ºæ£€ç´¢ç³»ç»Ÿé›†æˆ...")
    
    # æ£€æŸ¥å¯¼å…¥éƒ¨åˆ†
    if "ENHANCED_RETRIEVAL_AVAILABLE" not in content:
        # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¯¼å…¥
        import_section = '''
# å¢å¼ºæ£€ç´¢ç³»ç»Ÿå¯¼å…¥
try:
    from enhanced_retrieval_system import HybridRetriever, QueryProcessor, RelevanceReranker
    ENHANCED_RETRIEVAL_AVAILABLE = True
    print("âœ… å¢å¼ºæ£€ç´¢ç³»ç»Ÿå¯ç”¨")
except ImportError:
    ENHANCED_RETRIEVAL_AVAILABLE = False
    print("âš ï¸ å¢å¼ºæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨")
'''
        
        # æŸ¥æ‰¾åˆé€‚çš„æ’å…¥ä½ç½®ï¼ˆconfigå¯¼å…¥ä¹‹åï¼‰
        config_import_pos = content.find("from config import Config")
        if config_import_pos != -1:
            insert_pos = content.find('\n', config_import_pos) + 1
            content = content[:insert_pos] + import_section + content[insert_pos:]
            print("âœ… å·²æ·»åŠ å¢å¼ºæ£€ç´¢ç³»ç»Ÿå¯¼å…¥")
    
    # æ£€æŸ¥retrieve_documentsæ–¹æ³•
    method_patterns = [
        '"""æ£€ç´¢ç›¸å…³æ–‡æ¡£ - ä¼˜åŒ–ç‰ˆæœ¬"""',
        '"""æ£€ç´¢ç›¸å…³æ–‡æ¡£ - å¢å¼ºç‰ˆæœ¬"""',
        'def retrieve_documents(self, query: str) -> List[str]:'
    ]
    
    method_found = False
    for pattern in method_patterns:
        if pattern in content:
            method_found = True
            break
    
    if method_found:
        if 'å¢å¼ºæ£€ç´¢æŸ¥è¯¢' in content and 'ENHANCED_RETRIEVAL_AVAILABLE' in content:
            print("âœ… retrieve_documentsæ–¹æ³•å·²åŒ…å«å¢å¼ºæ£€ç´¢é€»è¾‘")
        else:
            print("âš ï¸ æ‰¾åˆ°retrieve_documentsæ–¹æ³•ä½†å¯èƒ½éœ€è¦æ›´æ–°")
    else:
        print("âŒ æœªæ‰¾åˆ°retrieve_documentsæ–¹æ³•")
        return False
    
    # ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ä¿®æ”¹ï¼‰
    if "ENHANCED_RETRIEVAL_AVAILABLE" in content:
        with open(rag_engine_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print("âœ… æ›´æ–°å·²ä¿å­˜")
    
    print("âœ… å¢å¼ºæ£€ç´¢ç³»ç»Ÿé›†æˆæ£€æŸ¥å®Œæˆ")
    return True

def main():
    """ä¸»å‡½æ•° - éƒ¨ç½²å¢å¼ºæ£€ç´¢ç³»ç»Ÿ"""
    print("ğŸš€ éƒ¨ç½²å¢å¼ºæ£€ç´¢ç³»ç»Ÿ")
    print("=" * 60)
    
    print("ğŸ“‹ å¢å¼ºæ£€ç´¢ç‰¹æ€§:")
    features = [
        "âœ… æŸ¥è¯¢æ‰©å±•å’ŒåŒä¹‰è¯å¤„ç†",
        "âœ… æ··åˆæ£€ç´¢ (å‘é‡ + å…³é”®è¯)",
        "âœ… å¤šæŸ¥è¯¢å˜ä½“æ£€ç´¢",
        "âœ… BM25å…³é”®è¯åŒ¹é…",
        "âœ… æ™ºèƒ½é‡æ’åºæœºåˆ¶",
        "âœ… é‡‘èé¢†åŸŸä¸“ä¸šä¼˜åŒ–",
        "âœ… è´¨é‡è¯„åˆ†å’Œè¿‡æ»¤"
    ]
    
    for feature in features:
        print(f"  {feature}")
    
    # é›†æˆåˆ°RAGå¼•æ“
    if integrate_enhanced_retrieval():
        print(f"\nğŸ‰ å¢å¼ºæ£€ç´¢ç³»ç»Ÿéƒ¨ç½²æˆåŠŸï¼")
        print(f"\nğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ:")
        improvements = [
            "æ£€ç´¢ç›¸å…³æ€§æå‡ 40-60%",
            "æŸ¥è¯¢è¦†ç›–ç‡æå‡ 30-50%", 
            "ä¸“ä¸šæœ¯è¯­åŒ¹é…å‡†ç¡®ç‡æå‡ 50%+",
            "å¤šè§’åº¦ä¿¡æ¯æ£€ç´¢",
            "æ›´æ™ºèƒ½çš„ç»“æœæ’åº"
        ]
        for imp in improvements:
            print(f"  ğŸ“Š {imp}")
        
        print(f"\nğŸ“‹ ä½¿ç”¨å»ºè®®:")
        suggestions = [
            "é‡æ–°è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯æ•ˆæœ",
            "è§‚å¯Ÿæ£€ç´¢ç»“æœçš„æ¥æºæ ‡è¯†",
            "æ£€æŸ¥æŸ¥è¯¢æ‰©å±•æ˜¯å¦ç”Ÿæ•ˆ",
            "å¯¹æ¯”å¢å¼ºå‰åçš„æ£€ç´¢è´¨é‡"
        ]
        for suggestion in suggestions:
            print(f"  ğŸ’¡ {suggestion}")
    else:
        print(f"\nâŒ å¢å¼ºæ£€ç´¢ç³»ç»Ÿéƒ¨ç½²å¤±è´¥")
    
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œæµ‹è¯•éªŒè¯æ£€ç´¢è´¨é‡æå‡æ•ˆæœ")

if __name__ == "__main__":
    main() 