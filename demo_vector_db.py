#!/usr/bin/env python
"""
å‘é‡æ•°æ®åº“åŠŸèƒ½æ¼”ç¤ºè„šæœ¬
å±•ç¤ºFAISSå‘é‡æ•°æ®åº“çš„æŒä¹…åŒ–å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½
"""

import os
import json
import numpy as np
from pathlib import Path
import time

def demo_vector_database():
    """æ¼”ç¤ºå‘é‡æ•°æ®åº“åŠŸèƒ½"""
    print("="*60)
    print("å‘é‡æ•°æ®åº“æŒä¹…åŒ–åŠŸèƒ½æ¼”ç¤º")
    print("="*60)
    
    try:
        import faiss
        print("âœ… FAISSåº“å¯¼å…¥æˆåŠŸ")
    except ImportError:
        print("âŒ FAISSåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install faiss-cpu")
        return
    
    from config import Config
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œå‘é‡æ•°æ®åº“ç›®å½•: {Config.VECTOR_DB_DIR}")
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“ç›®å½•
    vector_db_path = Path(Config.VECTOR_DB_DIR)
    vector_db_path.mkdir(exist_ok=True)
    print(f"âœ… å‘é‡æ•°æ®åº“ç›®å½•å·²åˆ›å»º: {vector_db_path}")
    
    # æ¼”ç¤ºåŸºæœ¬çš„FAISSåŠŸèƒ½
    print("\n" + "-"*40)
    print("æ¼”ç¤ºåŸºæœ¬FAISSå‘é‡ç´¢å¼•åŠŸèƒ½")
    print("-"*40)
    
    # åˆ›å»ºç¤ºä¾‹å‘é‡æ•°æ®
    dimension = 128  # ç®€åŒ–çš„å‘é‡ç»´åº¦
    num_vectors = 1000
    
    print(f"åˆ›å»º {num_vectors} ä¸ª {dimension} ç»´å‘é‡...")
    vectors = np.random.random((num_vectors, dimension)).astype(np.float32)
    
    # æ ‡å‡†åŒ–å‘é‡
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    vectors = vectors / norms
    print("âœ… å‘é‡æ ‡å‡†åŒ–å®Œæˆ")
    
    # åˆ›å»ºFAISSç´¢å¼•
    print("åˆ›å»ºFAISSç´¢å¼•...")
    index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç´¢å¼•
    index.add(vectors)
    print(f"âœ… FAISSç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {index.ntotal} ä¸ªå‘é‡")
    
    # æµ‹è¯•æœç´¢
    print("\næµ‹è¯•å‘é‡æœç´¢...")
    query_vector = np.random.random((1, dimension)).astype(np.float32)
    query_vector = query_vector / np.linalg.norm(query_vector)
    
    start_time = time.time()
    scores, indices = index.search(query_vector, 5)  # è¿”å›å‰5ä¸ªæœ€ç›¸ä¼¼çš„å‘é‡
    search_time = time.time() - start_time
    
    print(f"âœ… æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.4f}ç§’")
    print("æœç´¢ç»“æœ:")
    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
        print(f"  {i+1}. ç´¢å¼•: {idx}, ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
    
    # æ¼”ç¤ºæŒä¹…åŒ–å­˜å‚¨
    print("\n" + "-"*40)
    print("æ¼”ç¤ºå‘é‡æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨")
    print("-"*40)
    
    # ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
    index_file = vector_db_path / "demo_faiss_index.bin"
    metadata_file = vector_db_path / "demo_metadata.json"
    
    print(f"ä¿å­˜FAISSç´¢å¼•åˆ°: {index_file}")
    faiss.write_index(index, str(index_file))
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "total_vectors": int(index.ntotal),
        "vector_dimension": dimension,
        "index_type": "IndexFlatIP",
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "description": "æ¼”ç¤ºç”¨å‘é‡æ•°æ®åº“"
    }
    
    print(f"ä¿å­˜å…ƒæ•°æ®åˆ°: {metadata_file}")
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print("âœ… å‘é‡æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨å®Œæˆ")
    
    # æ¼”ç¤ºåŠ è½½æŒä¹…åŒ–æ•°æ®
    print("\n" + "-"*40)
    print("æ¼”ç¤ºä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“")
    print("-"*40)
    
    # æ¸…ç©ºå†…å­˜ä¸­çš„ç´¢å¼•
    del index
    print("âœ… å†…å­˜ä¸­çš„ç´¢å¼•å·²æ¸…ç©º")
    
    # ä»ç£ç›˜é‡æ–°åŠ è½½
    print(f"ä»ç£ç›˜åŠ è½½ç´¢å¼•: {index_file}")
    loaded_index = faiss.read_index(str(index_file))
    
    # åŠ è½½å…ƒæ•°æ®
    print(f"åŠ è½½å…ƒæ•°æ®: {metadata_file}")
    with open(metadata_file, 'r', encoding='utf-8') as f:
        loaded_metadata = json.load(f)
    
    print("âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ")
    print("åŠ è½½çš„å…ƒæ•°æ®:")
    for key, value in loaded_metadata.items():
        print(f"  {key}: {value}")
    
    # éªŒè¯åŠ è½½çš„ç´¢å¼•åŠŸèƒ½æ­£å¸¸
    print("\néªŒè¯åŠ è½½çš„ç´¢å¼•...")
    start_time = time.time()
    scores2, indices2 = loaded_index.search(query_vector, 5)
    search_time2 = time.time() - start_time
    
    print(f"âœ… æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time2:.4f}ç§’")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    if np.array_equal(scores, scores2) and np.array_equal(indices, indices2):
        print("âœ… åŠ è½½çš„ç´¢å¼•ä¸åŸå§‹ç´¢å¼•ç»“æœå®Œå…¨ä¸€è‡´")
    else:
        print("âŒ ç»“æœä¸ä¸€è‡´")
    
    print("\n" + "="*60)
    print("å‘é‡æ•°æ®åº“æŒä¹…åŒ–æ¼”ç¤ºå®Œæˆ")
    print("="*60)
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    for file_path in [index_file, metadata_file]:
        if file_path.exists():
            size = file_path.stat().st_size
            print(f"  {file_path}: {size:,} å­—èŠ‚")
    
    print(f"\nğŸ’¡ ä¼˜åŠ¿è¯´æ˜:")
    print(f"   - é¦–æ¬¡æ„å»ºï¼šè®¡ç®— {num_vectors} ä¸ªå‘é‡å¹¶ä¿å­˜")
    print(f"   - åç»­åŠ è½½ï¼šç›´æ¥ä»ç£ç›˜è¯»å–ï¼Œç§’çº§å¯åŠ¨")
    print(f"   - æœç´¢æ€§èƒ½ï¼š{num_vectors} ä¸ªå‘é‡æœç´¢è€—æ—¶ {search_time:.4f}ç§’")
    print(f"   - å­˜å‚¨ç©ºé—´ï¼šç´¢å¼•æ–‡ä»¶çº¦ {index_file.stat().st_size/1024:.1f} KB")
    
    return True

def demo_vector_database_integration():
    """æ¼”ç¤ºå‘é‡æ•°æ®åº“ä¸æ–‡æ¡£æ£€ç´¢çš„é›†æˆ"""
    print("\n" + "="*60)
    print("æ–‡æ¡£æ£€ç´¢é›†æˆæ¼”ç¤º")
    print("="*60)
    
    # æ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®
    documents = [
        "é“¶è¡Œåº”å½“å»ºç«‹å®Œå–„çš„é£é™©ç®¡ç†ä½“ç³»ï¼ŒåŒ…æ‹¬é£é™©è¯†åˆ«ã€é£é™©è¯„ä¼°ã€é£é™©æ§åˆ¶ç­‰ç¯èŠ‚ã€‚",
        "å•†ä¸šé“¶è¡Œçš„èµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº8%ï¼Œä¸€çº§èµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº6%ã€‚",
        "é“¶è¡Œä¸šé‡‘èæœºæ„åº”å½“å»ºç«‹å†…éƒ¨æ§åˆ¶åˆ¶åº¦ï¼Œç¡®ä¿ä¸šåŠ¡æ“ä½œçš„åˆè§„æ€§ã€‚",
        "é‡‘èæœºæ„åº”å½“å»ºç«‹å¥å…¨åæ´—é’±åˆ¶åº¦ï¼Œå±¥è¡Œåæ´—é’±ä¹‰åŠ¡ã€‚",
        "é“¶è¡Œåº”å½“å¯¹ä¿¡è´·èµ„äº§è¿›è¡Œåˆ†ç±»ç®¡ç†ï¼ŒåŠæ—¶è¯†åˆ«å’Œå¤„ç½®ä¸è‰¯èµ„äº§ã€‚"
    ]
    
    print(f"æ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®ï¼š{len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    
    # è¿™é‡Œä¼šåœ¨å®é™…ç³»ç»Ÿä¸­ä½¿ç”¨åµŒå…¥æ¨¡å‹ç”Ÿæˆå‘é‡
    # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºå‘é‡
    dimension = 768  # m3e-baseæ¨¡å‹çš„å‘é‡ç»´åº¦
    
    print("ç”Ÿæˆæ–‡æ¡£å‘é‡ï¼ˆå®é™…ç³»ç»Ÿä¸­ä¼šä½¿ç”¨m3e-baseæ¨¡å‹ï¼‰...")
    doc_vectors = np.random.random((len(documents), dimension)).astype(np.float32)
    # æ ‡å‡†åŒ–
    norms = np.linalg.norm(doc_vectors, axis=1, keepdims=True)
    doc_vectors = doc_vectors / norms
    
    # åˆ›å»ºæ–‡æ¡£ç´¢å¼•
    import faiss
    doc_index = faiss.IndexFlatIP(dimension)
    doc_index.add(doc_vectors)
    
    print(f"âœ… æ–‡æ¡£å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
    
    # æ¨¡æ‹ŸæŸ¥è¯¢
    queries = [
        "é“¶è¡Œé£é™©ç®¡ç†åˆ¶åº¦",
        "èµ„æœ¬å……è¶³ç‡è¦æ±‚",
        "åæ´—é’±è§„å®š"
    ]
    
    print("\næ–‡æ¡£æ£€ç´¢æ¼”ç¤º:")
    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå®é™…ç³»ç»Ÿä¸­ä¼šä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼‰
        query_vector = np.random.random((1, dimension)).astype(np.float32)
        query_vector = query_vector / np.linalg.norm(query_vector)
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        scores, indices = doc_index.search(query_vector, 3)
        
        print("æ£€ç´¢ç»“æœ:")
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(documents):
                print(f"  {i+1}. [åˆ†æ•°: {score:.4f}] {documents[idx]}")
    
    print("\nâœ… æ–‡æ¡£æ£€ç´¢æ¼”ç¤ºå®Œæˆ")

if __name__ == "__main__":
    try:
        success = demo_vector_database()
        if success:
            demo_vector_database_integration()
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 