"""
è¯Šæ–­åˆ‡ç‰‡ä¿®å¤é—®é¢˜çš„è„šæœ¬
æ£€æŸ¥ä¸ºä»€ä¹ˆä¿®å¤åè¿˜æ˜¯15598ä¸ªæ–‡æ¡£ç‰‡æ®µ
"""

import os
import sys
from vector_db import VectorDatabase
from config import Config
from config_improved import ImprovedConfig

def check_config_usage():
    """æ£€æŸ¥å½“å‰å®é™…ä½¿ç”¨çš„é…ç½®"""
    print("ğŸ” æ£€æŸ¥é…ç½®æ–‡ä»¶ä½¿ç”¨æƒ…å†µ")
    print("=" * 50)
    
    print("å½“å‰config.pyé…ç½®:")
    print(f"  CHUNK_SIZE: {Config.CHUNK_SIZE}")
    print(f"  CHUNK_OVERLAP: {Config.CHUNK_OVERLAP}")
    print(f"  TOP_K: {Config.TOP_K}")
    
    print("\nconfig_improved.pyé…ç½®:")
    print(f"  CHUNK_SIZE: {ImprovedConfig.CHUNK_SIZE}")
    print(f"  CHUNK_OVERLAP: {ImprovedConfig.CHUNK_OVERLAP}")
    print(f"  TOP_K: {ImprovedConfig.TOP_K}")
    
    # æ£€æŸ¥å“ªä¸ªé…ç½®è¢«å®é™…å¯¼å…¥
    print(f"\nå½“å‰Pythonæ¨¡å—ä¸­çš„config:")
    if 'config' in sys.modules:
        config_module = sys.modules['config']
        print(f"  æ¨¡å—è·¯å¾„: {config_module.__file__}")
        print(f"  CHUNK_SIZE: {getattr(config_module, 'Config', {}).CHUNK_SIZE if hasattr(config_module, 'Config') else 'æœªæ‰¾åˆ°'}")

def check_vector_db_files():
    """æ£€æŸ¥å‘é‡æ•°æ®åº“æ–‡ä»¶"""
    print("\nğŸ“ æ£€æŸ¥å‘é‡æ•°æ®åº“æ–‡ä»¶")
    print("=" * 50)
    
    if os.path.exists("vector_db"):
        files = os.listdir("vector_db")
        print(f"vector_dbç›®å½•ä¸­çš„æ–‡ä»¶: {files}")
        
        for file in files:
            if not file.startswith("demo_"):
                file_path = os.path.join("vector_db", file)
                if os.path.isfile(file_path):
                    size = os.path.getsize(file_path)
                    mtime = os.path.getmtime(file_path)
                    import datetime
                    mtime_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
                    print(f"  {file}: {size} bytes, ä¿®æ”¹æ—¶é—´: {mtime_str}")
    else:
        print("âŒ vector_dbç›®å½•ä¸å­˜åœ¨")

def analyze_chunk_distribution():
    """åˆ†æå½“å‰åˆ‡ç‰‡åˆ†å¸ƒ"""
    print("\nğŸ“Š åˆ†æå½“å‰åˆ‡ç‰‡åˆ†å¸ƒ")
    print("=" * 50)
    
    try:
        vdb = VectorDatabase()
        if not vdb.load_from_disk():
            print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
            return
        
        total_chunks = len(vdb.document_store)
        print(f"æ€»ç‰‡æ®µæ•°: {total_chunks}")
        
        # åˆ†æé•¿åº¦åˆ†å¸ƒ
        lengths = []
        for chunk_data in vdb.document_store.values():
            text = chunk_data.get('text', '')
            lengths.append(len(text))
        
        if lengths:
            avg_len = sum(lengths) / len(lengths)
            max_len = max(lengths)
            min_len = min(lengths)
            
            print(f"å¹³å‡é•¿åº¦: {avg_len:.1f} å­—ç¬¦")
            print(f"æœ€å¤§é•¿åº¦: {max_len} å­—ç¬¦")
            print(f"æœ€å°é•¿åº¦: {min_len} å­—ç¬¦")
            
            # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡
            ranges = {
                "0-10": len([l for l in lengths if 0 <= l <= 10]),
                "11-50": len([l for l in lengths if 11 <= l <= 50]),
                "51-200": len([l for l in lengths if 51 <= l <= 200]),
                "201-500": len([l for l in lengths if 201 <= l <= 500]),
                "501-1000": len([l for l in lengths if 501 <= l <= 1000]),
                "1000+": len([l for l in lengths if l > 1000])
            }
            
            print("\né•¿åº¦åˆ†å¸ƒ:")
            for range_name, count in ranges.items():
                percentage = (count / total_chunks) * 100
                print(f"  {range_name}: {count} ä¸ª ({percentage:.1f}%)")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡1000å­—ç¬¦çš„ç‰‡æ®µï¼ˆè¯´æ˜ä½¿ç”¨äº†æ–°é…ç½®ï¼‰
            long_chunks = ranges["1000+"]
            if long_chunks > 0:
                print(f"\nâœ… å‘ç° {long_chunks} ä¸ªè¶…è¿‡1000å­—ç¬¦çš„ç‰‡æ®µï¼Œè¯´æ˜ä½¿ç”¨äº†æ–°é…ç½®")
            else:
                print(f"\nâš ï¸ æ²¡æœ‰å‘ç°è¶…è¿‡1000å­—ç¬¦çš„ç‰‡æ®µï¼Œå¯èƒ½è¿˜åœ¨ä½¿ç”¨æ—§é…ç½®")
                
            # æ£€æŸ¥æœ€å¤§é•¿åº¦æ˜¯å¦æ¥è¿‘æ–°çš„CHUNK_SIZE
            if max_len > 800:
                print(f"âœ… æœ€å¤§ç‰‡æ®µé•¿åº¦ {max_len} æ¥è¿‘æ–°é…ç½®çš„CHUNK_SIZE (1000)")
            else:
                print(f"âš ï¸ æœ€å¤§ç‰‡æ®µé•¿åº¦ {max_len} ä»ç„¶æ¥è¿‘æ—§é…ç½®çš„CHUNK_SIZE (512)")
                
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

def check_duplicate_chunks():
    """æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é‡å¤åˆ‡ç‰‡"""
    print("\nğŸ” æ£€æŸ¥é‡å¤åˆ‡ç‰‡é—®é¢˜")
    print("=" * 50)
    
    try:
        vdb = VectorDatabase()
        if not vdb.load_from_disk():
            print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜åº¦ç›¸ä¼¼çš„ç‰‡æ®µ
        texts = []
        for chunk_data in vdb.document_store.values():
            text = chunk_data.get('text', '').strip()
            if text:
                texts.append(text)
        
        # å¯»æ‰¾ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µ
        similar_pairs = 0
        sample_similar = []
        
        for i in range(min(100, len(texts))):  # åªæ£€æŸ¥å‰100ä¸ªï¼Œé¿å…å¤ªæ…¢
            text1 = texts[i]
            for j in range(i+1, min(i+20, len(texts))):  # æ£€æŸ¥åç»­20ä¸ª
                text2 = texts[j]
                
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦ç›¸ä¼¼åº¦ï¼‰
                if len(text1) > 10 and len(text2) > 10:
                    common_length = 0
                    min_len = min(len(text1), len(text2))
                    
                    for k in range(min_len):
                        if text1[k] == text2[k]:
                            common_length += 1
                        else:
                            break
                    
                    similarity = common_length / min_len
                    if similarity > 0.8:  # 80%ç›¸ä¼¼
                        similar_pairs += 1
                        if len(sample_similar) < 5:
                            sample_similar.append((text1[:100], text2[:100], similarity))
        
        print(f"å‘ç° {similar_pairs} å¯¹é«˜åº¦ç›¸ä¼¼çš„ç‰‡æ®µ")
        
        if sample_similar:
            print("\nç›¸ä¼¼ç‰‡æ®µæ ·ä¾‹:")
            for i, (t1, t2, sim) in enumerate(sample_similar):
                print(f"  {i+1}. ç›¸ä¼¼åº¦: {sim:.2f}")
                print(f"     ç‰‡æ®µ1: {repr(t1)}...")
                print(f"     ç‰‡æ®µ2: {repr(t2)}...")
        
        if similar_pairs > total_chunks * 0.1:  # å¦‚æœè¶…è¿‡10%æ˜¯ç›¸ä¼¼çš„
            print("âš ï¸ æ£€æµ‹åˆ°å¤§é‡ç›¸ä¼¼ç‰‡æ®µï¼Œå¯èƒ½ä»ç„¶å­˜åœ¨é‡å¤åˆ‡ç‰‡é—®é¢˜")
        else:
            print("âœ… é‡å¤åˆ‡ç‰‡é—®é¢˜åŸºæœ¬è§£å†³")
            
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")

def test_new_chunking_algorithm():
    """æµ‹è¯•æ–°çš„åˆ‡ç‰‡ç®—æ³•å®é™…æ•ˆæœ"""
    print("\nğŸ§ª æµ‹è¯•æ–°åˆ‡ç‰‡ç®—æ³•")
    print("=" * 50)
    
    test_text = "é“¶è¡Œåº”å½“å»ºç«‹å®Œå–„çš„é£é™©ç®¡ç†ä½“ç³»ï¼ŒåŒ…æ‹¬é£é™©è¯†åˆ«ã€é£é™©è¯„ä¼°ã€é£é™©æ§åˆ¶ç­‰ç¯èŠ‚ã€‚å•†ä¸šé“¶è¡Œçš„èµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº8%ï¼Œä¸€çº§èµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº6%ï¼Œæ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡ä¸å¾—ä½äº5%ã€‚é“¶è¡Œåº”å½“æŒ‰ç…§ç›‘ç®¡è¦æ±‚ï¼Œå®šæœŸæŠ¥å‘Šèµ„æœ¬å……è¶³ç‡æƒ…å†µï¼Œç¡®ä¿èµ„æœ¬æ°´å¹³æ»¡è¶³ç›‘ç®¡æ ‡å‡†ã€‚åŒæ—¶ï¼Œé“¶è¡Œè¿˜åº”å½“å»ºç«‹èµ„æœ¬è§„åˆ’æœºåˆ¶ï¼Œåˆç†å®‰æ’èµ„æœ¬è¡¥å……è®¡åˆ’ã€‚" * 5  # é‡å¤5æ¬¡ï¼Œç¡®ä¿è¶…è¿‡1000å­—ç¬¦
    
    print(f"æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
    
    # ä½¿ç”¨å½“å‰é…ç½®è¿›è¡Œåˆ‡ç‰‡
    from vector_db import VectorDatabase
    
    # åˆ›å»ºä¸´æ—¶VectorDatabaseå®ä¾‹æ¥æµ‹è¯•åˆ‡ç‰‡
    vdb = VectorDatabase()
    chunks = vdb._split_document(test_text)
    
    print(f"\nåˆ‡ç‰‡ç»“æœ:")
    print(f"  æ€»ç‰‡æ®µæ•°: {len(chunks)}")
    
    for i, chunk in enumerate(chunks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"  ç‰‡æ®µ{i+1} (é•¿åº¦{len(chunk)}): {repr(chunk[:100])}...")
    
    # åˆ†æç‰‡æ®µé•¿åº¦
    chunk_lengths = [len(chunk) for chunk in chunks]
    if chunk_lengths:
        avg_len = sum(chunk_lengths) / len(chunk_lengths)
        max_len = max(chunk_lengths)
        print(f"\n  å¹³å‡ç‰‡æ®µé•¿åº¦: {avg_len:.1f}")
        print(f"  æœ€å¤§ç‰‡æ®µé•¿åº¦: {max_len}")
        
        if max_len > 800:
            print("âœ… åˆ‡ç‰‡é•¿åº¦ç¬¦åˆæ–°é…ç½®é¢„æœŸ")
        else:
            print("âš ï¸ åˆ‡ç‰‡é•¿åº¦ä»ç„¶åå°ï¼Œå¯èƒ½ä½¿ç”¨äº†æ—§é…ç½®")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ è¯Šæ–­åˆ‡ç‰‡ä¿®å¤é—®é¢˜")
    print("=" * 60)
    
    check_config_usage()
    check_vector_db_files()
    analyze_chunk_distribution()
    check_duplicate_chunks()
    test_new_chunking_algorithm()
    
    print("\nğŸ“‹ è¯Šæ–­æ€»ç»“:")
    print("1. å¦‚æœæœ€å¤§ç‰‡æ®µé•¿åº¦ä»ç„¶æ˜¯512ï¼Œè¯´æ˜è¿˜åœ¨ä½¿ç”¨æ—§é…ç½®")
    print("2. å¦‚æœå‘ç°å¤§é‡ç›¸ä¼¼ç‰‡æ®µï¼Œè¯´æ˜é‡å¤åˆ‡ç‰‡é—®é¢˜æœªè§£å†³")
    print("3. å¦‚æœæ–‡ä»¶ä¿®æ”¹æ—¶é—´å¾ˆæ—©ï¼Œè¯´æ˜å‘é‡æ•°æ®åº“æ²¡æœ‰é‡å»º")
    print("4. å¦‚æœ15598è¿™ä¸ªæ•°å­—æ²¡å˜ï¼Œè¯´æ˜é‡å»ºè¿‡ç¨‹å¯èƒ½å¤±è´¥äº†")

if __name__ == "__main__":
    main() 