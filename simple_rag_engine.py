#!/usr/bin/env python
"""
ç®€åŒ–ç‰ˆ RAG å¼•æ“ - é¿å… transformers å¹¶è¡Œå¤„ç†é—®é¢˜
ä¸“ä¸º ModelScope ç¯å¢ƒè®¾è®¡çš„æœ€å°åŒ–å®ç°
"""

import os
import torch
import gc
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

class SimpleRAGEngine:
    """ç®€åŒ–ç‰ˆ RAG å¼•æ“"""
    
    def __init__(self, config):
        """åˆå§‹åŒ–"""
        self.config = config
        self.llm = None
        self.tokenizer = None
        self.embedding_model = None
        self.index = None
        self.query_engine = None
        
    def load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹ - ä¼˜å…ˆæ–¹æ¡ˆ"""
        print(f"ğŸ“Š åŠ è½½åµŒå…¥æ¨¡å‹: {self.config.EMBEDDING_MODEL_PATH}")
        
        try:
            self.embedding_model = HuggingFaceEmbedding(
                model_name=self.config.EMBEDDING_MODEL_PATH,
                trust_remote_code=True
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_llm_simple(self):
        """ç®€å•åŠ è½½LLM - é¿å…å¤æ‚å¹¶è¡Œå¤„ç†"""
        print(f"ğŸ¤– ç®€å•åŠ è½½LLM: {self.config.LLM_MODEL_PATH}")
        
        try:
            # è®¾ç½®ç®€å•çš„ç¯å¢ƒå˜é‡
            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
            os.environ['TOKENIZERS_PARALLELISM'] = 'false'
            
            # åŠ è½½ tokenizer
            print("ğŸ“ åŠ è½½ tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.LLM_MODEL_PATH,
                trust_remote_code=True,
                use_fast=False  # é¿å…fast tokenizerçš„é—®é¢˜
            )
            print("âœ… tokenizer åŠ è½½æˆåŠŸ")
            
            # ç®€å•åŠ è½½æ¨¡å‹ - CPUæ¨¡å¼é¿å…GPUé—®é¢˜
            print("ğŸ”„ åŠ è½½æ¨¡å‹ (CPUæ¨¡å¼)...")
            self.llm = AutoModelForCausalLM.from_pretrained(
                self.config.LLM_MODEL_PATH,
                torch_dtype=torch.float32,  # ä½¿ç”¨float32é¿å…ç²¾åº¦é—®é¢˜
                device_map="cpu",           # å¼ºåˆ¶ä½¿ç”¨CPU
                trust_remote_code=True,
                low_cpu_mem_usage=True      # ä½å†…å­˜æ¨¡å¼
            )
            print("âœ… LLMæ¨¡å‹åŠ è½½æˆåŠŸ (CPUæ¨¡å¼)")
            return True
            
        except Exception as e:
            print(f"âŒ LLMåŠ è½½å¤±è´¥: {e}")
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æœ€åŸºæœ¬çš„åŠ è½½
            try:
                print("ğŸ”„ å°è¯•æœ€åŸºæœ¬çš„åŠ è½½æ–¹å¼...")
                self.llm = AutoModelForCausalLM.from_pretrained(
                    self.config.LLM_MODEL_PATH,
                    trust_remote_code=True
                )
                print("âœ… åŸºæœ¬æ¨¡å¼åŠ è½½æˆåŠŸ")
                return True
            except Exception as e2:
                print(f"âŒ åŸºæœ¬æ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                return False
    
    def initialize(self):
        """åˆå§‹åŒ–å¼•æ“"""
        print("ğŸš€ åˆå§‹åŒ–ç®€åŒ–ç‰ˆ RAG å¼•æ“...")
        
        # 1. åŠ è½½åµŒå…¥æ¨¡å‹
        if not self.load_embedding_model():
            print("âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½†ç»§ç»­å°è¯•...")
        
        # 2. åŠ è½½LLM (å¯é€‰)
        llm_loaded = self.load_llm_simple()
        if not llm_loaded:
            print("âš ï¸  LLMåŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨çº¯æ£€ç´¢æ¨¡å¼")
        
        # 3. è®¾ç½®å…¨å±€é…ç½®
        if self.embedding_model:
            Settings.embed_model = self.embedding_model
        
        print("âœ… RAG å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        return True
    
    def create_documents(self, texts: List[str]) -> List[Document]:
        """åˆ›å»ºæ–‡æ¡£"""
        print(f"ğŸ“„ åˆ›å»º {len(texts)} ä¸ªæ–‡æ¡£...")
        documents = [Document(text=text) for text in texts]
        print("âœ… æ–‡æ¡£åˆ›å»ºå®Œæˆ")
        return documents
    
    def build_index(self, documents: List[Document]):
        """æ„å»ºç´¢å¼•"""
        print("ğŸ” æ„å»ºå‘é‡ç´¢å¼•...")
        
        try:
            self.index = VectorStoreIndex.from_documents(documents)
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=getattr(self.config, 'TOP_K', 3),
                response_mode="compact"
            )
            print("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False
    
    def generate_simple_answer(self, query: str, retrieved_texts: List[str]) -> str:
        """ç®€å•ç”Ÿæˆç­”æ¡ˆ"""
        if not self.llm or not self.tokenizer:
            # å¦‚æœæ²¡æœ‰LLMï¼Œè¿”å›æ£€ç´¢åˆ°çš„æ–‡æœ¬æ‘˜è¦
            if retrieved_texts:
                return f"æ ¹æ®ç›¸å…³æ–‡æ¡£ï¼Œ{query}çš„ç›¸å…³ä¿¡æ¯å¦‚ä¸‹ï¼š\n" + "\n".join(retrieved_texts[:2])
            else:
                return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
        try:
            # æ„å»ºç®€å•æç¤ºè¯
            context = "\n".join(retrieved_texts[:2])  # åªå–å‰2ä¸ªç»“æœé¿å…è¿‡é•¿
            prompt = f"é—®é¢˜ï¼š{query}\nå‚è€ƒå†…å®¹ï¼š{context}\nå›ç­”ï¼š"
            
            # ç®€å•ç”Ÿæˆ
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=512, truncation=True)
            
            with torch.no_grad():
                outputs = self.llm.generate(
                    inputs,
                    max_new_tokens=256,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            answer = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            return answer.strip() if answer.strip() else "æ— æ³•ç”Ÿæˆç­”æ¡ˆ"
            
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆå¤±è´¥ï¼Œè¿”å›æ£€ç´¢ç»“æœ: {e}")
            if retrieved_texts:
                return f"æ ¹æ®æ–‡æ¡£å†…å®¹ï¼š{retrieved_texts[0][:200]}..."
            else:
                return "å¤„ç†å‡ºé”™"
    
    def query(self, question: str) -> Dict[str, Any]:
        """æŸ¥è¯¢æ¥å£"""
        try:
            print(f"ğŸ” æŸ¥è¯¢: {question}")
            
            if not self.query_engine:
                return {
                    "question": question,
                    "answer": "ç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–",
                    "retrieved_texts": [],
                    "confidence": 0.0
                }
            
            # æ£€ç´¢
            response = self.query_engine.query(question)
            
            # æå–æ£€ç´¢æ–‡æœ¬
            retrieved_texts = []
            if hasattr(response, 'source_nodes'):
                retrieved_texts = [node.text for node in response.source_nodes]
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = self.generate_simple_answer(question, retrieved_texts)
            
            return {
                "question": question,
                "answer": answer,
                "retrieved_texts": retrieved_texts,
                "confidence": 0.8
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "question": question,
                "answer": f"æŸ¥è¯¢é”™è¯¯: {str(e)}",
                "retrieved_texts": [],
                "confidence": 0.0
            }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ§¹ æ¸…ç†èµ„æº...")
        
        if self.llm:
            del self.llm
            self.llm = None
        
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        
        if self.embedding_model:
            del self.embedding_model
            self.embedding_model = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")

# æµ‹è¯•å‡½æ•°
def test_simple_engine():
    """æµ‹è¯•ç®€åŒ–å¼•æ“"""
    print("="*60)
    print("ğŸ§ª æµ‹è¯•ç®€åŒ–ç‰ˆ RAG å¼•æ“")
    print("="*60)
    
    try:
        # æ¨¡æ‹Ÿé…ç½®
        class SimpleConfig:
            LLM_MODEL_PATH = "/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-7B-Instruct"
            EMBEDDING_MODEL_PATH = "/mnt/workspace/.cache/modelscope/models/Jerry0/m3e-base"
            TOP_K = 3
        
        # åˆ›å»ºå¼•æ“
        engine = SimpleRAGEngine(SimpleConfig)
        
        # åˆå§‹åŒ–
        if not engine.initialize():
            print("âŒ åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # æµ‹è¯•æ–‡æ¡£
        test_docs = [
            "é“¶è¡Œèµ„æœ¬å……è¶³ç‡è¦æ±‚ï¼šæ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡5%ï¼Œä¸€çº§èµ„æœ¬å……è¶³ç‡6%ï¼Œèµ„æœ¬å……è¶³ç‡8%ã€‚",
            "åŸå•†è¡Œå†…å®¡éƒ¨é—¨è´Ÿè´£äººéœ€åœ¨5ä¸ªå·¥ä½œæ—¥å†…å¤‡æ¡ˆã€‚",
            "ä¸ªäººç†è´¢äº§å“éœ€è¦é£é™©è¯„ä¼°å’ŒåŒ¹é…ã€‚"
        ]
        
        # æ„å»ºç´¢å¼•
        documents = engine.create_documents(test_docs)
        if not engine.build_index(documents):
            print("âŒ ç´¢å¼•æ„å»ºå¤±è´¥")
            return False
        
        # æµ‹è¯•æŸ¥è¯¢
        result = engine.query("é“¶è¡Œèµ„æœ¬å……è¶³ç‡è¦æ±‚æ˜¯å¤šå°‘ï¼Ÿ")
        print(f"\nâœ… æµ‹è¯•ç»“æœ:")
        print(f"é—®é¢˜: {result['question']}")
        print(f"ç­”æ¡ˆ: {result['answer']}")
        
        # æ¸…ç†
        engine.cleanup()
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_simple_engine() 